2024-09-13 19:06:23,406:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-13 19:06:23,406:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-13 19:06:23,406:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-13 19:06:23,406:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-13 19:14:18,207:WARNING:C:\Users\jonat\AppData\Local\Temp\ipykernel_10880\2966898723.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.
  mergeddata = pd.read_csv('dataset_20240907_plusP_Psummary_tfidf.csv')

2024-09-13 19:50:02,263:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-13 19:50:02,263:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-13 19:50:02,263:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-13 19:50:02,263:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-13 19:50:48,227:INFO:PyCaret ClassificationExperiment
2024-09-13 19:50:48,227:INFO:Logging name: clf-default-name
2024-09-13 19:50:48,227:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-13 19:50:48,228:INFO:version 3.3.2
2024-09-13 19:50:48,228:INFO:Initializing setup()
2024-09-13 19:50:48,228:INFO:self.USI: 6ce8
2024-09-13 19:50:48,228:INFO:self._variable_keys: {'X_test', 'memory', 'fold_groups_param', 'fix_imbalance', 'exp_id', 'X', 'gpu_n_jobs_param', 'log_plots_param', 'target_param', 'y', 'X_train', 'fold_generator', 'is_multiclass', 'y_train', '_ml_usecase', 'logging_param', 'html_param', 'y_test', 'USI', 'seed', 'fold_shuffle_param', 'gpu_param', 'idx', 'n_jobs_param', 'exp_name_log', 'pipeline', 'data', '_available_plots'}
2024-09-13 19:50:48,228:INFO:Checking environment
2024-09-13 19:50:48,229:INFO:python_version: 3.11.9
2024-09-13 19:50:48,229:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-09-13 19:50:48,231:INFO:machine: AMD64
2024-09-13 19:50:48,232:INFO:platform: Windows-10-10.0.19045-SP0
2024-09-13 19:50:48,247:INFO:Memory: svmem(total=6302064640, available=575729664, percent=90.9, used=5726334976, free=575729664)
2024-09-13 19:50:48,252:INFO:Physical Core: 2
2024-09-13 19:50:48,252:INFO:Logical Core: 4
2024-09-13 19:50:48,252:INFO:Checking libraries
2024-09-13 19:50:48,252:INFO:System:
2024-09-13 19:50:48,252:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-09-13 19:50:48,252:INFO:executable: c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Scripts\python.exe
2024-09-13 19:50:48,252:INFO:   machine: Windows-10-10.0.19045-SP0
2024-09-13 19:50:48,254:INFO:PyCaret required dependencies:
2024-09-13 19:50:48,600:INFO:                 pip: 24.2
2024-09-13 19:50:48,600:INFO:          setuptools: 74.1.2
2024-09-13 19:50:48,600:INFO:             pycaret: 3.3.2
2024-09-13 19:50:48,600:INFO:             IPython: 8.27.0
2024-09-13 19:50:48,600:INFO:          ipywidgets: 8.1.5
2024-09-13 19:50:48,600:INFO:                tqdm: 4.66.5
2024-09-13 19:50:48,608:INFO:               numpy: 1.26.4
2024-09-13 19:50:48,608:INFO:              pandas: 2.1.4
2024-09-13 19:50:48,610:INFO:              jinja2: 3.1.4
2024-09-13 19:50:48,610:INFO:               scipy: 1.11.4
2024-09-13 19:50:48,610:INFO:              joblib: 1.3.2
2024-09-13 19:50:48,610:INFO:             sklearn: 1.4.2
2024-09-13 19:50:48,610:INFO:                pyod: 2.0.2
2024-09-13 19:50:48,610:INFO:            imblearn: 0.12.3
2024-09-13 19:50:48,610:INFO:   category_encoders: 2.6.3
2024-09-13 19:50:48,610:INFO:            lightgbm: 4.5.0
2024-09-13 19:50:48,610:INFO:               numba: 0.60.0
2024-09-13 19:50:48,610:INFO:            requests: 2.32.3
2024-09-13 19:50:48,610:INFO:          matplotlib: 3.7.5
2024-09-13 19:50:48,610:INFO:          scikitplot: 0.3.7
2024-09-13 19:50:48,610:INFO:         yellowbrick: 1.5
2024-09-13 19:50:48,610:INFO:              plotly: 5.24.1
2024-09-13 19:50:48,610:INFO:    plotly-resampler: Not installed
2024-09-13 19:50:48,610:INFO:             kaleido: 0.2.1
2024-09-13 19:50:48,610:INFO:           schemdraw: 0.15
2024-09-13 19:50:48,610:INFO:         statsmodels: 0.14.2
2024-09-13 19:50:48,610:INFO:              sktime: 0.26.0
2024-09-13 19:50:48,610:INFO:               tbats: 1.1.3
2024-09-13 19:50:48,610:INFO:            pmdarima: 2.0.4
2024-09-13 19:50:48,610:INFO:              psutil: 6.0.0
2024-09-13 19:50:48,610:INFO:          markupsafe: 2.1.5
2024-09-13 19:50:48,610:INFO:             pickle5: Not installed
2024-09-13 19:50:48,610:INFO:         cloudpickle: 3.0.0
2024-09-13 19:50:48,610:INFO:         deprecation: 2.1.0
2024-09-13 19:50:48,610:INFO:              xxhash: 3.5.0
2024-09-13 19:50:48,610:INFO:           wurlitzer: Not installed
2024-09-13 19:50:48,610:INFO:PyCaret optional dependencies:
2024-09-13 19:50:48,732:INFO:                shap: Not installed
2024-09-13 19:50:48,732:INFO:           interpret: Not installed
2024-09-13 19:50:48,732:INFO:                umap: Not installed
2024-09-13 19:50:48,732:INFO:     ydata_profiling: Not installed
2024-09-13 19:50:48,732:INFO:  explainerdashboard: Not installed
2024-09-13 19:50:48,732:INFO:             autoviz: Not installed
2024-09-13 19:50:48,732:INFO:           fairlearn: Not installed
2024-09-13 19:50:48,732:INFO:          deepchecks: Not installed
2024-09-13 19:50:48,732:INFO:             xgboost: Not installed
2024-09-13 19:50:48,732:INFO:            catboost: Not installed
2024-09-13 19:50:48,732:INFO:              kmodes: Not installed
2024-09-13 19:50:48,732:INFO:             mlxtend: Not installed
2024-09-13 19:50:48,732:INFO:       statsforecast: Not installed
2024-09-13 19:50:48,732:INFO:        tune_sklearn: Not installed
2024-09-13 19:50:48,732:INFO:                 ray: Not installed
2024-09-13 19:50:48,732:INFO:            hyperopt: Not installed
2024-09-13 19:50:48,732:INFO:              optuna: Not installed
2024-09-13 19:50:48,732:INFO:               skopt: Not installed
2024-09-13 19:50:48,732:INFO:              mlflow: Not installed
2024-09-13 19:50:48,732:INFO:              gradio: Not installed
2024-09-13 19:50:48,732:INFO:             fastapi: Not installed
2024-09-13 19:50:48,732:INFO:             uvicorn: Not installed
2024-09-13 19:50:48,732:INFO:              m2cgen: Not installed
2024-09-13 19:50:48,732:INFO:           evidently: Not installed
2024-09-13 19:50:48,732:INFO:               fugue: Not installed
2024-09-13 19:50:48,732:INFO:           streamlit: Not installed
2024-09-13 19:50:48,732:INFO:             prophet: Not installed
2024-09-13 19:50:48,732:INFO:None
2024-09-13 19:50:48,732:INFO:Set up data.
2024-09-13 19:51:07,085:INFO:PyCaret ClassificationExperiment
2024-09-13 19:51:07,086:INFO:Logging name: clf-default-name
2024-09-13 19:51:07,087:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-13 19:51:07,087:INFO:version 3.3.2
2024-09-13 19:51:07,087:INFO:Initializing setup()
2024-09-13 19:51:07,087:INFO:self.USI: 05da
2024-09-13 19:51:07,087:INFO:self._variable_keys: {'X_test', 'memory', 'fold_groups_param', 'fix_imbalance', 'exp_id', 'X', 'gpu_n_jobs_param', 'log_plots_param', 'target_param', 'y', 'X_train', 'fold_generator', 'is_multiclass', 'y_train', '_ml_usecase', 'logging_param', 'html_param', 'y_test', 'USI', 'seed', 'fold_shuffle_param', 'gpu_param', 'idx', 'n_jobs_param', 'exp_name_log', 'pipeline', 'data', '_available_plots'}
2024-09-13 19:51:07,088:INFO:Checking environment
2024-09-13 19:51:07,088:INFO:python_version: 3.11.9
2024-09-13 19:51:07,088:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-09-13 19:51:07,089:INFO:machine: AMD64
2024-09-13 19:51:07,089:INFO:platform: Windows-10-10.0.19045-SP0
2024-09-13 19:51:07,093:INFO:Memory: svmem(total=6302064640, available=703176704, percent=88.8, used=5598887936, free=703176704)
2024-09-13 19:51:07,094:INFO:Physical Core: 2
2024-09-13 19:51:07,094:INFO:Logical Core: 4
2024-09-13 19:51:07,094:INFO:Checking libraries
2024-09-13 19:51:07,094:INFO:System:
2024-09-13 19:51:07,094:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-09-13 19:51:07,094:INFO:executable: c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Scripts\python.exe
2024-09-13 19:51:07,095:INFO:   machine: Windows-10-10.0.19045-SP0
2024-09-13 19:51:07,095:INFO:PyCaret required dependencies:
2024-09-13 19:51:07,095:INFO:                 pip: 24.2
2024-09-13 19:51:07,095:INFO:          setuptools: 74.1.2
2024-09-13 19:51:07,095:INFO:             pycaret: 3.3.2
2024-09-13 19:51:07,095:INFO:             IPython: 8.27.0
2024-09-13 19:51:07,096:INFO:          ipywidgets: 8.1.5
2024-09-13 19:51:07,096:INFO:                tqdm: 4.66.5
2024-09-13 19:51:07,096:INFO:               numpy: 1.26.4
2024-09-13 19:51:07,096:INFO:              pandas: 2.1.4
2024-09-13 19:51:07,096:INFO:              jinja2: 3.1.4
2024-09-13 19:51:07,096:INFO:               scipy: 1.11.4
2024-09-13 19:51:07,096:INFO:              joblib: 1.3.2
2024-09-13 19:51:07,096:INFO:             sklearn: 1.4.2
2024-09-13 19:51:07,097:INFO:                pyod: 2.0.2
2024-09-13 19:51:07,097:INFO:            imblearn: 0.12.3
2024-09-13 19:51:07,097:INFO:   category_encoders: 2.6.3
2024-09-13 19:51:07,097:INFO:            lightgbm: 4.5.0
2024-09-13 19:51:07,097:INFO:               numba: 0.60.0
2024-09-13 19:51:07,098:INFO:            requests: 2.32.3
2024-09-13 19:51:07,098:INFO:          matplotlib: 3.7.5
2024-09-13 19:51:07,098:INFO:          scikitplot: 0.3.7
2024-09-13 19:51:07,098:INFO:         yellowbrick: 1.5
2024-09-13 19:51:07,098:INFO:              plotly: 5.24.1
2024-09-13 19:51:07,098:INFO:    plotly-resampler: Not installed
2024-09-13 19:51:07,098:INFO:             kaleido: 0.2.1
2024-09-13 19:51:07,099:INFO:           schemdraw: 0.15
2024-09-13 19:51:07,099:INFO:         statsmodels: 0.14.2
2024-09-13 19:51:07,099:INFO:              sktime: 0.26.0
2024-09-13 19:51:07,099:INFO:               tbats: 1.1.3
2024-09-13 19:51:07,100:INFO:            pmdarima: 2.0.4
2024-09-13 19:51:07,100:INFO:              psutil: 6.0.0
2024-09-13 19:51:07,100:INFO:          markupsafe: 2.1.5
2024-09-13 19:51:07,100:INFO:             pickle5: Not installed
2024-09-13 19:51:07,101:INFO:         cloudpickle: 3.0.0
2024-09-13 19:51:07,101:INFO:         deprecation: 2.1.0
2024-09-13 19:51:07,101:INFO:              xxhash: 3.5.0
2024-09-13 19:51:07,101:INFO:           wurlitzer: Not installed
2024-09-13 19:51:07,101:INFO:PyCaret optional dependencies:
2024-09-13 19:51:07,101:INFO:                shap: Not installed
2024-09-13 19:51:07,101:INFO:           interpret: Not installed
2024-09-13 19:51:07,102:INFO:                umap: Not installed
2024-09-13 19:51:07,102:INFO:     ydata_profiling: Not installed
2024-09-13 19:51:07,102:INFO:  explainerdashboard: Not installed
2024-09-13 19:51:07,102:INFO:             autoviz: Not installed
2024-09-13 19:51:07,102:INFO:           fairlearn: Not installed
2024-09-13 19:51:07,102:INFO:          deepchecks: Not installed
2024-09-13 19:51:07,102:INFO:             xgboost: Not installed
2024-09-13 19:51:07,102:INFO:            catboost: Not installed
2024-09-13 19:51:07,102:INFO:              kmodes: Not installed
2024-09-13 19:51:07,102:INFO:             mlxtend: Not installed
2024-09-13 19:51:07,103:INFO:       statsforecast: Not installed
2024-09-13 19:51:07,103:INFO:        tune_sklearn: Not installed
2024-09-13 19:51:07,103:INFO:                 ray: Not installed
2024-09-13 19:51:07,103:INFO:            hyperopt: Not installed
2024-09-13 19:51:07,103:INFO:              optuna: Not installed
2024-09-13 19:51:07,103:INFO:               skopt: Not installed
2024-09-13 19:51:07,103:INFO:              mlflow: Not installed
2024-09-13 19:51:07,103:INFO:              gradio: Not installed
2024-09-13 19:51:07,103:INFO:             fastapi: Not installed
2024-09-13 19:51:07,103:INFO:             uvicorn: Not installed
2024-09-13 19:51:07,103:INFO:              m2cgen: Not installed
2024-09-13 19:51:07,104:INFO:           evidently: Not installed
2024-09-13 19:51:07,104:INFO:               fugue: Not installed
2024-09-13 19:51:07,104:INFO:           streamlit: Not installed
2024-09-13 19:51:07,104:INFO:             prophet: Not installed
2024-09-13 19:51:07,104:INFO:None
2024-09-13 19:51:07,104:INFO:Set up data.
2024-09-13 19:51:09,308:INFO:Set up folding strategy.
2024-09-13 19:51:09,308:INFO:Set up train/test split.
2024-09-13 19:51:10,974:INFO:Set up index.
2024-09-13 19:51:11,015:INFO:Assigning column types.
2024-09-13 19:51:11,906:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-13 19:51:11,991:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-13 19:51:11,991:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-13 19:51:12,063:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:12,063:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:12,125:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-13 19:51:12,125:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-13 19:51:12,194:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:12,194:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:12,194:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-13 19:51:12,264:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-13 19:51:12,327:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:12,327:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:12,397:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-13 19:51:12,445:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:12,445:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:12,445:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-13 19:51:12,580:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:12,580:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:12,712:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:12,712:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:12,727:INFO:Preparing preprocessing pipeline...
2024-09-13 19:51:12,913:INFO:Set up simple imputation.
2024-09-13 19:51:12,976:INFO:Set up column name cleaning.
2024-09-13 19:51:17,912:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:249: UserWarning: Persisting input arguments took 1.96s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  fitted_transformer = self._memory_fit(

2024-09-13 19:51:28,207:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:256: UserWarning: Persisting input arguments took 1.65s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_transform(

2024-09-13 19:51:29,080:INFO:Finished creating preprocessing pipeline.
2024-09-13 19:51:29,107:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\jonat\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'version',
                                             'assigned_to_detail.id',
                                             'Psummary_intermittent',
                                             'Psummary_bug',
                                             'Psummary_tracking',
                                             'Psummary_single', 'Psummary_sync',
                                             'Psummary_pr', 'Psummary_wptsync',
                                             'Psummary_test', 'Psummary_wpt',
                                             'Psumma...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-13 19:51:29,107:INFO:Creating final display dataframe.
2024-09-13 19:51:35,529:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:289: UserWarning: Persisting input arguments took 1.96s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-09-13 19:51:47,325:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:111: UserWarning: Persisting input arguments took 1.55s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = pipeline._memory_transform(transformer, X, y)

2024-09-13 19:51:49,926:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:289: UserWarning: Persisting input arguments took 1.79s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-09-13 19:51:54,905:INFO:Setup _display_container:                     Description             Value
0                    Session id              6887
1                        Target          severity
2                   Target type        Multiclass
3           Original data shape      (4109, 4184)
4        Transformed data shape      (4109, 4184)
5   Transformed train set shape      (2876, 4184)
6    Transformed test set shape      (1233, 4184)
7              Numeric features              3132
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              05da
2024-09-13 19:51:55,119:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:55,119:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:55,248:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:55,248:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-13 19:51:55,248:INFO:setup() successfully completed in 48.16s...............
2024-09-13 19:51:55,248:INFO:Initializing compare_models()
2024-09-13 19:51:55,248:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-13 19:51:55,248:INFO:Checking exceptions
2024-09-13 19:51:55,893:INFO:Preparing display monitor
2024-09-13 19:51:55,979:INFO:Initializing Logistic Regression
2024-09-13 19:51:55,980:INFO:Total runtime is 1.661380132039388e-05 minutes
2024-09-13 19:51:55,996:INFO:SubProcess create_model() called ==================================
2024-09-13 19:51:55,997:INFO:Initializing create_model()
2024-09-13 19:51:55,997:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEE0CE5610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 19:51:55,997:INFO:Checking exceptions
2024-09-13 19:51:55,998:INFO:Importing libraries
2024-09-13 19:51:55,998:INFO:Copying training dataset
2024-09-13 19:51:57,659:INFO:Defining folds
2024-09-13 19:51:57,659:INFO:Declaring metric variables
2024-09-13 19:51:57,661:INFO:Importing untrained model
2024-09-13 19:51:57,675:INFO:Logistic Regression Imported successfully
2024-09-13 19:51:57,695:INFO:Starting cross validation
2024-09-13 19:51:57,725:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 19:51:57,855:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 19:52:11,753:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\dask\dataframe\__init__.py:42: FutureWarning: 
Dask dataframe query planning is disabled because dask-expr is not installed.

You can install it with `pip install dask[dataframe]` or `conda install dask`.
This will raise in a future version.

  warnings.warn(msg, FutureWarning)

2024-09-13 19:52:12,799:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\dask\dataframe\__init__.py:42: FutureWarning: 
Dask dataframe query planning is disabled because dask-expr is not installed.

You can install it with `pip install dask[dataframe]` or `conda install dask`.
This will raise in a future version.

  warnings.warn(msg, FutureWarning)

2024-09-13 19:52:15,176:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\dask\dataframe\__init__.py:42: FutureWarning: 
Dask dataframe query planning is disabled because dask-expr is not installed.

You can install it with `pip install dask[dataframe]` or `conda install dask`.
This will raise in a future version.

  warnings.warn(msg, FutureWarning)

2024-09-13 19:52:16,318:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\dask\dataframe\__init__.py:42: FutureWarning: 
Dask dataframe query planning is disabled because dask-expr is not installed.

You can install it with `pip install dask[dataframe]` or `conda install dask`.
This will raise in a future version.

  warnings.warn(msg, FutureWarning)

2024-09-13 19:54:52,780:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 19:54:53,569:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 19:54:54,828:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 19:54:55,164:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 19:55:04,347:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 19:55:05,029:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 19:55:06,603:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 19:55:06,691:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 19:55:06,841:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 19:55:06,874:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 19:57:24,805:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 19:57:25,904:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 19:57:28,084:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 19:57:28,839:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 19:57:37,099:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 19:57:37,114:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 19:57:38,148:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 19:57:38,166:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 19:57:40,046:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 19:57:40,056:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 19:57:40,605:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 19:57:40,615:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 19:58:59,456:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 19:58:59,540:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 19:59:07,283:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 19:59:07,333:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 19:59:07,344:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 19:59:07,423:INFO:Calculating mean and std
2024-09-13 19:59:07,507:INFO:Creating metrics dataframe
2024-09-13 19:59:07,607:INFO:Uploading results into container
2024-09-13 19:59:07,607:INFO:Uploading model into container now
2024-09-13 19:59:07,631:INFO:_master_model_container: 1
2024-09-13 19:59:07,631:INFO:_display_container: 2
2024-09-13 19:59:07,641:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=6887, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-13 19:59:07,642:INFO:create_model() successfully completed......................................
2024-09-13 19:59:08,690:INFO:SubProcess create_model() end ==================================
2024-09-13 19:59:08,695:INFO:Creating metrics dataframe
2024-09-13 19:59:08,706:INFO:Initializing K Neighbors Classifier
2024-09-13 19:59:08,706:INFO:Total runtime is 7.212129672368367 minutes
2024-09-13 19:59:08,706:INFO:SubProcess create_model() called ==================================
2024-09-13 19:59:08,706:INFO:Initializing create_model()
2024-09-13 19:59:08,706:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEE0CE5610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 19:59:08,706:INFO:Checking exceptions
2024-09-13 19:59:08,706:INFO:Importing libraries
2024-09-13 19:59:08,706:INFO:Copying training dataset
2024-09-13 19:59:10,187:INFO:Defining folds
2024-09-13 19:59:10,187:INFO:Declaring metric variables
2024-09-13 19:59:10,195:INFO:Importing untrained model
2024-09-13 19:59:10,207:INFO:K Neighbors Classifier Imported successfully
2024-09-13 19:59:10,224:INFO:Starting cross validation
2024-09-13 19:59:10,257:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 19:59:10,262:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 19:59:39,718:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 19:59:40,500:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 19:59:41,410:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 19:59:42,396:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:00:12,418:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:00:12,989:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:00:13,877:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:00:14,331:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:00:34,600:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:00:34,833:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:00:34,873:INFO:Calculating mean and std
2024-09-13 20:00:34,873:INFO:Creating metrics dataframe
2024-09-13 20:00:34,901:INFO:Uploading results into container
2024-09-13 20:00:34,901:INFO:Uploading model into container now
2024-09-13 20:00:34,901:INFO:_master_model_container: 2
2024-09-13 20:00:34,901:INFO:_display_container: 2
2024-09-13 20:00:34,901:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-13 20:00:34,901:INFO:create_model() successfully completed......................................
2024-09-13 20:00:35,267:INFO:SubProcess create_model() end ==================================
2024-09-13 20:00:35,267:INFO:Creating metrics dataframe
2024-09-13 20:00:35,283:INFO:Initializing Naive Bayes
2024-09-13 20:00:35,283:INFO:Total runtime is 8.655077866713206 minutes
2024-09-13 20:00:35,300:INFO:SubProcess create_model() called ==================================
2024-09-13 20:00:35,300:INFO:Initializing create_model()
2024-09-13 20:00:35,300:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEE0CE5610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 20:00:35,300:INFO:Checking exceptions
2024-09-13 20:00:35,300:INFO:Importing libraries
2024-09-13 20:00:35,300:INFO:Copying training dataset
2024-09-13 20:00:36,367:INFO:Defining folds
2024-09-13 20:00:36,367:INFO:Declaring metric variables
2024-09-13 20:00:36,384:INFO:Importing untrained model
2024-09-13 20:00:36,387:INFO:Naive Bayes Imported successfully
2024-09-13 20:00:36,407:INFO:Starting cross validation
2024-09-13 20:00:36,443:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 20:00:36,450:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 20:01:05,277:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:01:05,287:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:01:05,653:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:01:05,671:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:01:07,250:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:01:07,264:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:01:07,736:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:01:32,586:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:01:33,466:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:01:34,404:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:01:34,414:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:01:35,120:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:01:35,129:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:01:51,437:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:01:51,447:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:01:51,725:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:01:51,728:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:01:51,756:INFO:Calculating mean and std
2024-09-13 20:01:51,761:INFO:Creating metrics dataframe
2024-09-13 20:01:51,761:INFO:Uploading results into container
2024-09-13 20:01:51,761:INFO:Uploading model into container now
2024-09-13 20:01:51,761:INFO:_master_model_container: 3
2024-09-13 20:01:51,761:INFO:_display_container: 2
2024-09-13 20:01:51,761:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-13 20:01:51,761:INFO:create_model() successfully completed......................................
2024-09-13 20:01:51,994:INFO:SubProcess create_model() end ==================================
2024-09-13 20:01:51,994:INFO:Creating metrics dataframe
2024-09-13 20:01:52,011:INFO:Initializing Decision Tree Classifier
2024-09-13 20:01:52,011:INFO:Total runtime is 9.933871467908224 minutes
2024-09-13 20:01:52,011:INFO:SubProcess create_model() called ==================================
2024-09-13 20:01:52,011:INFO:Initializing create_model()
2024-09-13 20:01:52,027:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEE0CE5610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 20:01:52,027:INFO:Checking exceptions
2024-09-13 20:01:52,027:INFO:Importing libraries
2024-09-13 20:01:52,028:INFO:Copying training dataset
2024-09-13 20:01:53,110:INFO:Defining folds
2024-09-13 20:01:53,110:INFO:Declaring metric variables
2024-09-13 20:01:53,119:INFO:Importing untrained model
2024-09-13 20:01:53,130:INFO:Decision Tree Classifier Imported successfully
2024-09-13 20:01:53,146:INFO:Starting cross validation
2024-09-13 20:01:53,181:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 20:01:53,186:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 20:02:19,801:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:02:21,151:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:02:21,356:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:02:46,774:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:02:48,691:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:02:49,132:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:02:49,902:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:03:05,689:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:03:06,539:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:03:06,573:INFO:Calculating mean and std
2024-09-13 20:03:06,573:INFO:Creating metrics dataframe
2024-09-13 20:03:06,573:INFO:Uploading results into container
2024-09-13 20:03:06,582:INFO:Uploading model into container now
2024-09-13 20:03:06,585:INFO:_master_model_container: 4
2024-09-13 20:03:06,585:INFO:_display_container: 2
2024-09-13 20:03:06,588:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=6887, splitter='best')
2024-09-13 20:03:06,590:INFO:create_model() successfully completed......................................
2024-09-13 20:03:06,789:INFO:SubProcess create_model() end ==================================
2024-09-13 20:03:06,789:INFO:Creating metrics dataframe
2024-09-13 20:03:06,805:INFO:Initializing SVM - Linear Kernel
2024-09-13 20:03:06,805:INFO:Total runtime is 11.180446334679923 minutes
2024-09-13 20:03:06,828:INFO:SubProcess create_model() called ==================================
2024-09-13 20:03:06,831:INFO:Initializing create_model()
2024-09-13 20:03:06,831:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEE0CE5610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 20:03:06,831:INFO:Checking exceptions
2024-09-13 20:03:06,831:INFO:Importing libraries
2024-09-13 20:03:06,831:INFO:Copying training dataset
2024-09-13 20:03:08,009:INFO:Defining folds
2024-09-13 20:03:08,010:INFO:Declaring metric variables
2024-09-13 20:03:08,015:INFO:Importing untrained model
2024-09-13 20:03:08,025:INFO:SVM - Linear Kernel Imported successfully
2024-09-13 20:03:08,044:INFO:Starting cross validation
2024-09-13 20:03:08,071:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 20:03:08,078:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 20:04:03,843:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:04:03,853:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:04:04,726:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:04:04,737:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:04:05,308:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:04:05,323:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:04:06,346:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:04:06,355:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:05:04,609:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:05:04,619:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:05:06,474:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:05:06,515:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:05:07,218:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:05:07,233:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:05:07,239:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:05:09,503:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:05:09,513:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:05:42,687:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:05:42,696:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:05:45,038:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:05:45,048:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:05:45,089:INFO:Calculating mean and std
2024-09-13 20:05:45,089:INFO:Creating metrics dataframe
2024-09-13 20:05:45,122:INFO:Uploading results into container
2024-09-13 20:05:45,127:INFO:Uploading model into container now
2024-09-13 20:05:45,127:INFO:_master_model_container: 5
2024-09-13 20:05:45,127:INFO:_display_container: 2
2024-09-13 20:05:45,127:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=6887, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-13 20:05:45,127:INFO:create_model() successfully completed......................................
2024-09-13 20:05:45,502:INFO:SubProcess create_model() end ==================================
2024-09-13 20:05:45,502:INFO:Creating metrics dataframe
2024-09-13 20:05:45,522:INFO:Initializing Ridge Classifier
2024-09-13 20:05:45,522:INFO:Total runtime is 13.825731575489046 minutes
2024-09-13 20:05:45,533:INFO:SubProcess create_model() called ==================================
2024-09-13 20:05:45,533:INFO:Initializing create_model()
2024-09-13 20:05:45,533:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEE0CE5610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 20:05:45,533:INFO:Checking exceptions
2024-09-13 20:05:45,533:INFO:Importing libraries
2024-09-13 20:05:45,533:INFO:Copying training dataset
2024-09-13 20:05:47,015:INFO:Defining folds
2024-09-13 20:05:47,015:INFO:Declaring metric variables
2024-09-13 20:05:47,029:INFO:Importing untrained model
2024-09-13 20:05:47,034:INFO:Ridge Classifier Imported successfully
2024-09-13 20:05:47,060:INFO:Starting cross validation
2024-09-13 20:05:47,091:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 20:05:47,099:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 20:06:27,669:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:06:27,695:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:06:28,561:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:06:28,561:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:06:29,491:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:06:30,907:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:06:30,945:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:07:19,331:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:07:19,341:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:07:20,233:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:07:20,274:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:07:20,682:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:07:23,023:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:08:04,984:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:08:05,138:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:08:05,267:INFO:Calculating mean and std
2024-09-13 20:08:05,293:INFO:Creating metrics dataframe
2024-09-13 20:08:05,355:INFO:Uploading results into container
2024-09-13 20:08:05,358:INFO:Uploading model into container now
2024-09-13 20:08:05,362:INFO:_master_model_container: 6
2024-09-13 20:08:05,362:INFO:_display_container: 2
2024-09-13 20:08:05,364:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=6887, solver='auto',
                tol=0.0001)
2024-09-13 20:08:05,364:INFO:create_model() successfully completed......................................
2024-09-13 20:08:06,284:INFO:SubProcess create_model() end ==================================
2024-09-13 20:08:06,284:INFO:Creating metrics dataframe
2024-09-13 20:08:06,300:INFO:Initializing Random Forest Classifier
2024-09-13 20:08:06,300:INFO:Total runtime is 16.172024007638296 minutes
2024-09-13 20:08:06,318:INFO:SubProcess create_model() called ==================================
2024-09-13 20:08:06,319:INFO:Initializing create_model()
2024-09-13 20:08:06,320:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEE0CE5610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 20:08:06,320:INFO:Checking exceptions
2024-09-13 20:08:06,320:INFO:Importing libraries
2024-09-13 20:08:06,321:INFO:Copying training dataset
2024-09-13 20:08:08,307:INFO:Defining folds
2024-09-13 20:08:08,307:INFO:Declaring metric variables
2024-09-13 20:08:08,322:INFO:Importing untrained model
2024-09-13 20:08:08,344:INFO:Random Forest Classifier Imported successfully
2024-09-13 20:08:08,381:INFO:Starting cross validation
2024-09-13 20:08:08,440:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 20:08:08,450:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 20:08:51,869:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:08:51,880:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:08:53,088:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:08:54,380:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:08:55,722:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:09:32,547:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:09:34,648:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:09:36,061:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:09:37,291:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:09:59,395:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:09:59,403:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:10:00,662:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:10:00,699:INFO:Calculating mean and std
2024-09-13 20:10:00,725:INFO:Creating metrics dataframe
2024-09-13 20:10:00,773:INFO:Uploading results into container
2024-09-13 20:10:00,775:INFO:Uploading model into container now
2024-09-13 20:10:00,775:INFO:_master_model_container: 7
2024-09-13 20:10:00,775:INFO:_display_container: 2
2024-09-13 20:10:00,775:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6887, verbose=0,
                       warm_start=False)
2024-09-13 20:10:00,775:INFO:create_model() successfully completed......................................
2024-09-13 20:10:01,208:INFO:SubProcess create_model() end ==================================
2024-09-13 20:10:01,208:INFO:Creating metrics dataframe
2024-09-13 20:10:01,241:INFO:Initializing Quadratic Discriminant Analysis
2024-09-13 20:10:01,241:INFO:Total runtime is 18.08771588007609 minutes
2024-09-13 20:10:01,241:INFO:SubProcess create_model() called ==================================
2024-09-13 20:10:01,241:INFO:Initializing create_model()
2024-09-13 20:10:01,241:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEE0CE5610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 20:10:01,241:INFO:Checking exceptions
2024-09-13 20:10:01,241:INFO:Importing libraries
2024-09-13 20:10:01,241:INFO:Copying training dataset
2024-09-13 20:10:02,858:INFO:Defining folds
2024-09-13 20:10:02,858:INFO:Declaring metric variables
2024-09-13 20:10:02,867:INFO:Importing untrained model
2024-09-13 20:10:02,879:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-13 20:10:02,899:INFO:Starting cross validation
2024-09-13 20:10:02,932:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 20:10:02,939:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 20:10:32,697:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 20:10:34,314:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 20:10:35,084:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 20:10:37,162:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 20:11:26,640:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:11:27,456:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:11:27,495:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:11:27,841:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:11:27,865:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:11:27,872:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:11:28,490:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:11:28,501:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:11:45,412:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 20:11:46,026:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 20:11:46,893:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 20:11:47,119:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 20:12:26,519:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:12:26,536:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:12:27,523:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:12:27,572:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:12:27,592:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:12:28,288:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:12:36,470:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 20:12:36,683:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 20:12:55,613:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:12:55,825:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:12:55,903:INFO:Calculating mean and std
2024-09-13 20:12:56,007:INFO:Creating metrics dataframe
2024-09-13 20:12:56,070:INFO:Uploading results into container
2024-09-13 20:12:56,078:INFO:Uploading model into container now
2024-09-13 20:12:56,087:INFO:_master_model_container: 8
2024-09-13 20:12:56,087:INFO:_display_container: 2
2024-09-13 20:12:56,091:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-13 20:12:56,091:INFO:create_model() successfully completed......................................
2024-09-13 20:12:56,749:INFO:SubProcess create_model() end ==================================
2024-09-13 20:12:56,749:INFO:Creating metrics dataframe
2024-09-13 20:12:56,779:INFO:Initializing Ada Boost Classifier
2024-09-13 20:12:56,779:INFO:Total runtime is 21.013339738051094 minutes
2024-09-13 20:12:56,787:INFO:SubProcess create_model() called ==================================
2024-09-13 20:12:56,788:INFO:Initializing create_model()
2024-09-13 20:12:56,788:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEE0CE5610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 20:12:56,788:INFO:Checking exceptions
2024-09-13 20:12:56,788:INFO:Importing libraries
2024-09-13 20:12:56,790:INFO:Copying training dataset
2024-09-13 20:12:58,141:INFO:Defining folds
2024-09-13 20:12:58,141:INFO:Declaring metric variables
2024-09-13 20:12:58,152:INFO:Importing untrained model
2024-09-13 20:12:58,159:INFO:Ada Boost Classifier Imported successfully
2024-09-13 20:12:58,175:INFO:Starting cross validation
2024-09-13 20:12:58,206:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 20:12:58,214:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 20:13:13,255:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 20:13:13,720:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 20:13:14,233:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 20:13:49,842:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:13:49,927:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:13:50,868:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:13:50,882:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:13:52,420:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:13:52,432:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:13:52,593:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:13:52,602:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:14:24,889:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 20:14:25,730:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 20:14:26,442:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 20:14:28,481:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 20:15:04,607:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:15:04,618:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:15:05,536:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:15:05,551:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:15:05,965:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:15:05,976:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:15:06,851:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:15:06,872:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:15:19,342:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 20:15:20,163:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 20:15:40,138:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:15:40,146:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:15:41,045:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:15:41,050:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:15:41,121:INFO:Calculating mean and std
2024-09-13 20:15:41,214:INFO:Creating metrics dataframe
2024-09-13 20:15:41,329:INFO:Uploading results into container
2024-09-13 20:15:41,335:INFO:Uploading model into container now
2024-09-13 20:15:41,343:INFO:_master_model_container: 9
2024-09-13 20:15:41,344:INFO:_display_container: 2
2024-09-13 20:15:41,346:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=6887)
2024-09-13 20:15:41,347:INFO:create_model() successfully completed......................................
2024-09-13 20:15:42,662:INFO:SubProcess create_model() end ==================================
2024-09-13 20:15:42,662:INFO:Creating metrics dataframe
2024-09-13 20:15:42,698:INFO:Initializing Gradient Boosting Classifier
2024-09-13 20:15:42,698:INFO:Total runtime is 23.77866120735804 minutes
2024-09-13 20:15:42,708:INFO:SubProcess create_model() called ==================================
2024-09-13 20:15:42,708:INFO:Initializing create_model()
2024-09-13 20:15:42,709:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEE0CE5610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 20:15:42,709:INFO:Checking exceptions
2024-09-13 20:15:42,709:INFO:Importing libraries
2024-09-13 20:15:42,710:INFO:Copying training dataset
2024-09-13 20:15:44,720:INFO:Defining folds
2024-09-13 20:15:44,720:INFO:Declaring metric variables
2024-09-13 20:15:44,732:INFO:Importing untrained model
2024-09-13 20:15:44,740:INFO:Gradient Boosting Classifier Imported successfully
2024-09-13 20:15:44,756:INFO:Starting cross validation
2024-09-13 20:15:44,792:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 20:15:44,802:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 20:18:58,824:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:19:00,145:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:19:01,135:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:19:01,154:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:19:01,474:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:22:08,187:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:22:08,195:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:22:09,558:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:22:09,569:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:22:09,914:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:22:11,958:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:24:20,156:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:24:21,485:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:24:21,540:INFO:Calculating mean and std
2024-09-13 20:24:21,602:INFO:Creating metrics dataframe
2024-09-13 20:24:21,675:INFO:Uploading results into container
2024-09-13 20:24:21,680:INFO:Uploading model into container now
2024-09-13 20:24:21,686:INFO:_master_model_container: 10
2024-09-13 20:24:21,687:INFO:_display_container: 2
2024-09-13 20:24:21,691:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6887, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-13 20:24:21,692:INFO:create_model() successfully completed......................................
2024-09-13 20:24:23,122:INFO:SubProcess create_model() end ==================================
2024-09-13 20:24:23,122:INFO:Creating metrics dataframe
2024-09-13 20:24:23,163:INFO:Initializing Linear Discriminant Analysis
2024-09-13 20:24:23,163:INFO:Total runtime is 32.45308121442795 minutes
2024-09-13 20:24:23,173:INFO:SubProcess create_model() called ==================================
2024-09-13 20:24:23,174:INFO:Initializing create_model()
2024-09-13 20:24:23,174:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEE0CE5610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 20:24:23,174:INFO:Checking exceptions
2024-09-13 20:24:23,175:INFO:Importing libraries
2024-09-13 20:24:23,176:INFO:Copying training dataset
2024-09-13 20:24:24,683:INFO:Defining folds
2024-09-13 20:24:24,684:INFO:Declaring metric variables
2024-09-13 20:24:24,697:INFO:Importing untrained model
2024-09-13 20:24:24,704:INFO:Linear Discriminant Analysis Imported successfully
2024-09-13 20:24:24,722:INFO:Starting cross validation
2024-09-13 20:24:24,768:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 20:24:24,783:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 20:26:48,028:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:26:48,040:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:26:48,054:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:26:48,102:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:26:48,727:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:26:48,764:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:26:51,194:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:28:46,759:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:28:48,859:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:28:50,528:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:28:50,566:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:28:51,917:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:28:51,924:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:30:21,727:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:30:21,730:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:30:22,277:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 20:30:22,303:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:30:22,534:INFO:Calculating mean and std
2024-09-13 20:30:22,696:INFO:Creating metrics dataframe
2024-09-13 20:30:22,856:INFO:Uploading results into container
2024-09-13 20:30:22,865:INFO:Uploading model into container now
2024-09-13 20:30:22,887:INFO:_master_model_container: 11
2024-09-13 20:30:22,887:INFO:_display_container: 2
2024-09-13 20:30:22,893:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-13 20:30:22,894:INFO:create_model() successfully completed......................................
2024-09-13 20:30:24,778:INFO:SubProcess create_model() end ==================================
2024-09-13 20:30:24,778:INFO:Creating metrics dataframe
2024-09-13 20:30:24,817:INFO:Initializing Extra Trees Classifier
2024-09-13 20:30:24,817:INFO:Total runtime is 38.480641253789265 minutes
2024-09-13 20:30:24,827:INFO:SubProcess create_model() called ==================================
2024-09-13 20:30:24,828:INFO:Initializing create_model()
2024-09-13 20:30:24,829:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEE0CE5610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 20:30:24,829:INFO:Checking exceptions
2024-09-13 20:30:24,829:INFO:Importing libraries
2024-09-13 20:30:24,833:INFO:Copying training dataset
2024-09-13 20:30:27,070:INFO:Defining folds
2024-09-13 20:30:27,071:INFO:Declaring metric variables
2024-09-13 20:30:27,081:INFO:Importing untrained model
2024-09-13 20:30:27,123:INFO:Extra Trees Classifier Imported successfully
2024-09-13 20:30:27,188:INFO:Starting cross validation
2024-09-13 20:30:27,280:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 20:30:27,323:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 20:31:25,680:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:31:25,712:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:31:26,452:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:31:26,671:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:31:28,884:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:32:16,908:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:32:17,954:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:32:18,342:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:32:18,853:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:33:00,559:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:33:01,488:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:33:01,592:INFO:Calculating mean and std
2024-09-13 20:33:01,718:INFO:Creating metrics dataframe
2024-09-13 20:33:01,833:INFO:Uploading results into container
2024-09-13 20:33:01,842:INFO:Uploading model into container now
2024-09-13 20:33:01,867:INFO:_master_model_container: 12
2024-09-13 20:33:01,867:INFO:_display_container: 2
2024-09-13 20:33:01,873:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=6887, verbose=0,
                     warm_start=False)
2024-09-13 20:33:01,873:INFO:create_model() successfully completed......................................
2024-09-13 20:33:03,463:INFO:SubProcess create_model() end ==================================
2024-09-13 20:33:03,464:INFO:Creating metrics dataframe
2024-09-13 20:33:03,492:INFO:Initializing Light Gradient Boosting Machine
2024-09-13 20:33:03,492:INFO:Total runtime is 41.12522848447164 minutes
2024-09-13 20:33:03,499:INFO:SubProcess create_model() called ==================================
2024-09-13 20:33:03,500:INFO:Initializing create_model()
2024-09-13 20:33:03,501:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEE0CE5610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 20:33:03,501:INFO:Checking exceptions
2024-09-13 20:33:03,502:INFO:Importing libraries
2024-09-13 20:33:03,503:INFO:Copying training dataset
2024-09-13 20:33:05,280:INFO:Defining folds
2024-09-13 20:33:05,280:INFO:Declaring metric variables
2024-09-13 20:33:05,290:INFO:Importing untrained model
2024-09-13 20:33:05,302:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-13 20:33:05,325:INFO:Starting cross validation
2024-09-13 20:33:05,372:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 20:33:05,381:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 20:33:45,783:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:33:46,413:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:33:47,423:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:33:52,319:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:34:21,245:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:34:21,347:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:34:21,790:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:34:35,775:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:34:47,013:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:34:47,421:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:34:47,498:INFO:Calculating mean and std
2024-09-13 20:34:47,566:INFO:Creating metrics dataframe
2024-09-13 20:34:47,636:INFO:Uploading results into container
2024-09-13 20:34:47,642:INFO:Uploading model into container now
2024-09-13 20:34:47,649:INFO:_master_model_container: 13
2024-09-13 20:34:47,649:INFO:_display_container: 2
2024-09-13 20:34:47,653:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6887, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-13 20:34:47,653:INFO:create_model() successfully completed......................................
2024-09-13 20:34:48,848:INFO:SubProcess create_model() end ==================================
2024-09-13 20:34:48,848:INFO:Creating metrics dataframe
2024-09-13 20:34:48,888:INFO:Initializing Dummy Classifier
2024-09-13 20:34:48,888:INFO:Total runtime is 42.88182354370753 minutes
2024-09-13 20:34:48,900:INFO:SubProcess create_model() called ==================================
2024-09-13 20:34:48,901:INFO:Initializing create_model()
2024-09-13 20:34:48,901:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEE0CE5610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 20:34:48,903:INFO:Checking exceptions
2024-09-13 20:34:48,903:INFO:Importing libraries
2024-09-13 20:34:48,907:INFO:Copying training dataset
2024-09-13 20:34:50,232:INFO:Defining folds
2024-09-13 20:34:50,232:INFO:Declaring metric variables
2024-09-13 20:34:50,240:INFO:Importing untrained model
2024-09-13 20:34:50,253:INFO:Dummy Classifier Imported successfully
2024-09-13 20:34:50,269:INFO:Starting cross validation
2024-09-13 20:34:50,318:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 20:34:50,329:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 20:35:26,635:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:35:26,643:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:35:27,642:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:35:27,653:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:35:28,335:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:35:28,371:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:35:30,015:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:36:10,328:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:36:11,560:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:36:11,820:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:36:11,831:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:36:13,207:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:36:13,221:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:36:34,351:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:36:34,360:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:36:35,065:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 20:36:35,071:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 20:36:35,112:INFO:Calculating mean and std
2024-09-13 20:36:35,116:INFO:Creating metrics dataframe
2024-09-13 20:36:35,123:INFO:Uploading results into container
2024-09-13 20:36:35,125:INFO:Uploading model into container now
2024-09-13 20:36:35,128:INFO:_master_model_container: 14
2024-09-13 20:36:35,128:INFO:_display_container: 2
2024-09-13 20:36:35,128:INFO:DummyClassifier(constant=None, random_state=6887, strategy='prior')
2024-09-13 20:36:35,129:INFO:create_model() successfully completed......................................
2024-09-13 20:36:35,425:INFO:SubProcess create_model() end ==================================
2024-09-13 20:36:35,425:INFO:Creating metrics dataframe
2024-09-13 20:36:35,477:INFO:Initializing create_model()
2024-09-13 20:36:35,477:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6887, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 20:36:35,478:INFO:Checking exceptions
2024-09-13 20:36:35,490:INFO:Importing libraries
2024-09-13 20:36:35,491:INFO:Copying training dataset
2024-09-13 20:36:37,189:INFO:Defining folds
2024-09-13 20:36:37,189:INFO:Declaring metric variables
2024-09-13 20:36:37,190:INFO:Importing untrained model
2024-09-13 20:36:37,190:INFO:Declaring custom model
2024-09-13 20:36:37,192:INFO:Random Forest Classifier Imported successfully
2024-09-13 20:36:37,219:INFO:Cross validation set to False
2024-09-13 20:36:37,219:INFO:Fitting Model
2024-09-13 20:36:49,635:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6887, verbose=0,
                       warm_start=False)
2024-09-13 20:36:49,635:INFO:create_model() successfully completed......................................
2024-09-13 20:36:49,936:INFO:_master_model_container: 14
2024-09-13 20:36:49,936:INFO:_display_container: 2
2024-09-13 20:36:49,937:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6887, verbose=0,
                       warm_start=False)
2024-09-13 20:36:49,937:INFO:compare_models() successfully completed......................................
2024-09-13 21:43:14,793:INFO:Initializing compare_models()
2024-09-13 21:43:14,813:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-13 21:43:14,816:INFO:Checking exceptions
2024-09-13 21:43:18,152:INFO:Preparing display monitor
2024-09-13 21:43:18,818:INFO:Initializing Logistic Regression
2024-09-13 21:43:18,819:INFO:Total runtime is 6.6681702931722e-05 minutes
2024-09-13 21:43:18,850:INFO:SubProcess create_model() called ==================================
2024-09-13 21:43:18,946:INFO:Initializing create_model()
2024-09-13 21:43:18,946:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CE810570D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 21:43:18,947:INFO:Checking exceptions
2024-09-13 21:43:18,947:INFO:Importing libraries
2024-09-13 21:43:18,948:INFO:Copying training dataset
2024-09-13 21:43:28,851:INFO:Defining folds
2024-09-13 21:43:28,851:INFO:Declaring metric variables
2024-09-13 21:43:28,874:INFO:Importing untrained model
2024-09-13 21:43:28,903:INFO:Logistic Regression Imported successfully
2024-09-13 21:43:28,937:INFO:Starting cross validation
2024-09-13 21:43:29,013:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 21:43:29,047:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 21:46:33,552:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 21:46:34,962:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 21:46:35,249:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 21:46:35,798:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 21:46:47,234:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 21:46:49,641:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 21:46:50,655:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 21:46:50,716:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:46:51,683:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 21:46:51,701:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:49:09,754:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 21:49:10,862:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 21:49:12,011:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 21:49:12,549:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 21:49:21,924:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 21:49:21,944:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:49:23,102:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 21:49:23,123:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:49:24,404:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 21:49:24,418:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:49:25,109:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 21:49:25,120:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:51:44,073:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 21:51:44,582:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 21:51:58,306:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 21:51:59,059:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 21:51:59,088:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:51:59,342:INFO:Calculating mean and std
2024-09-13 21:51:59,575:INFO:Creating metrics dataframe
2024-09-13 21:51:59,867:INFO:Uploading results into container
2024-09-13 21:51:59,886:INFO:Uploading model into container now
2024-09-13 21:51:59,917:INFO:_master_model_container: 15
2024-09-13 21:51:59,917:INFO:_display_container: 3
2024-09-13 21:51:59,927:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=6887, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-13 21:51:59,927:INFO:create_model() successfully completed......................................
2024-09-13 21:52:03,062:INFO:SubProcess create_model() end ==================================
2024-09-13 21:52:03,062:INFO:Creating metrics dataframe
2024-09-13 21:52:03,110:INFO:Initializing K Neighbors Classifier
2024-09-13 21:52:03,110:INFO:Total runtime is 8.73825320402781 minutes
2024-09-13 21:52:03,132:INFO:SubProcess create_model() called ==================================
2024-09-13 21:52:03,132:INFO:Initializing create_model()
2024-09-13 21:52:03,132:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CE810570D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 21:52:03,132:INFO:Checking exceptions
2024-09-13 21:52:03,132:INFO:Importing libraries
2024-09-13 21:52:03,144:INFO:Copying training dataset
2024-09-13 21:52:10,294:INFO:Defining folds
2024-09-13 21:52:10,295:INFO:Declaring metric variables
2024-09-13 21:52:10,308:INFO:Importing untrained model
2024-09-13 21:52:10,330:INFO:K Neighbors Classifier Imported successfully
2024-09-13 21:52:10,351:INFO:Starting cross validation
2024-09-13 21:52:10,424:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 21:52:10,452:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 21:53:32,331:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:53:32,645:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:53:32,861:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:53:33,355:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:54:10,152:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:54:10,508:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:54:12,538:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:54:13,374:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:54:35,623:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:54:35,672:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:54:35,825:INFO:Calculating mean and std
2024-09-13 21:54:35,978:INFO:Creating metrics dataframe
2024-09-13 21:54:36,129:INFO:Uploading results into container
2024-09-13 21:54:36,145:INFO:Uploading model into container now
2024-09-13 21:54:36,163:INFO:_master_model_container: 16
2024-09-13 21:54:36,163:INFO:_display_container: 3
2024-09-13 21:54:36,175:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-13 21:54:36,175:INFO:create_model() successfully completed......................................
2024-09-13 21:54:38,162:INFO:SubProcess create_model() end ==================================
2024-09-13 21:54:38,162:INFO:Creating metrics dataframe
2024-09-13 21:54:38,199:INFO:Initializing Naive Bayes
2024-09-13 21:54:38,200:INFO:Total runtime is 11.32307793299357 minutes
2024-09-13 21:54:38,208:INFO:SubProcess create_model() called ==================================
2024-09-13 21:54:38,209:INFO:Initializing create_model()
2024-09-13 21:54:38,209:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CE810570D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 21:54:38,210:INFO:Checking exceptions
2024-09-13 21:54:38,210:INFO:Importing libraries
2024-09-13 21:54:38,212:INFO:Copying training dataset
2024-09-13 21:54:39,961:INFO:Defining folds
2024-09-13 21:54:39,961:INFO:Declaring metric variables
2024-09-13 21:54:39,979:INFO:Importing untrained model
2024-09-13 21:54:39,987:INFO:Naive Bayes Imported successfully
2024-09-13 21:54:40,015:INFO:Starting cross validation
2024-09-13 21:54:40,036:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 21:54:40,043:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 21:55:16,818:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:55:16,856:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:55:16,950:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:55:16,965:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:55:19,035:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:55:19,561:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:55:19,590:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:55:51,290:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:55:54,693:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:55:55,782:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:55:55,792:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:55:56,801:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:55:56,814:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:56:15,740:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:56:15,754:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:56:17,482:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:56:17,492:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:56:17,571:INFO:Calculating mean and std
2024-09-13 21:56:17,671:INFO:Creating metrics dataframe
2024-09-13 21:56:17,771:INFO:Uploading results into container
2024-09-13 21:56:17,779:INFO:Uploading model into container now
2024-09-13 21:56:17,789:INFO:_master_model_container: 17
2024-09-13 21:56:17,789:INFO:_display_container: 3
2024-09-13 21:56:17,793:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-13 21:56:17,793:INFO:create_model() successfully completed......................................
2024-09-13 21:56:18,837:INFO:SubProcess create_model() end ==================================
2024-09-13 21:56:18,837:INFO:Creating metrics dataframe
2024-09-13 21:56:18,856:INFO:Initializing Decision Tree Classifier
2024-09-13 21:56:18,856:INFO:Total runtime is 13.000692486763 minutes
2024-09-13 21:56:18,865:INFO:SubProcess create_model() called ==================================
2024-09-13 21:56:18,865:INFO:Initializing create_model()
2024-09-13 21:56:18,865:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CE810570D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 21:56:18,871:INFO:Checking exceptions
2024-09-13 21:56:18,871:INFO:Importing libraries
2024-09-13 21:56:18,871:INFO:Copying training dataset
2024-09-13 21:56:20,454:INFO:Defining folds
2024-09-13 21:56:20,454:INFO:Declaring metric variables
2024-09-13 21:56:20,475:INFO:Importing untrained model
2024-09-13 21:56:20,477:INFO:Decision Tree Classifier Imported successfully
2024-09-13 21:56:20,511:INFO:Starting cross validation
2024-09-13 21:56:20,543:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 21:56:20,554:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 21:57:04,053:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:57:05,553:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:57:06,329:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:57:38,321:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:57:40,478:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:57:40,703:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:57:41,041:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:58:07,558:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:58:08,952:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 21:58:09,122:INFO:Calculating mean and std
2024-09-13 21:58:09,279:INFO:Creating metrics dataframe
2024-09-13 21:58:09,424:INFO:Uploading results into container
2024-09-13 21:58:09,430:INFO:Uploading model into container now
2024-09-13 21:58:09,449:INFO:_master_model_container: 18
2024-09-13 21:58:09,449:INFO:_display_container: 3
2024-09-13 21:58:09,464:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=6887, splitter='best')
2024-09-13 21:58:09,465:INFO:create_model() successfully completed......................................
2024-09-13 21:58:11,748:INFO:SubProcess create_model() end ==================================
2024-09-13 21:58:11,748:INFO:Creating metrics dataframe
2024-09-13 21:58:11,798:INFO:Initializing SVM - Linear Kernel
2024-09-13 21:58:11,799:INFO:Total runtime is 14.883045824368795 minutes
2024-09-13 21:58:11,809:INFO:SubProcess create_model() called ==================================
2024-09-13 21:58:11,810:INFO:Initializing create_model()
2024-09-13 21:58:11,810:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CE810570D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 21:58:11,811:INFO:Checking exceptions
2024-09-13 21:58:11,811:INFO:Importing libraries
2024-09-13 21:58:11,812:INFO:Copying training dataset
2024-09-13 21:58:13,929:INFO:Defining folds
2024-09-13 21:58:13,929:INFO:Declaring metric variables
2024-09-13 21:58:13,948:INFO:Importing untrained model
2024-09-13 21:58:13,965:INFO:SVM - Linear Kernel Imported successfully
2024-09-13 21:58:13,980:INFO:Starting cross validation
2024-09-13 21:58:14,009:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 21:58:14,018:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 21:59:21,602:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 21:59:21,673:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:59:23,437:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 21:59:23,456:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:59:25,638:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 21:59:25,655:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 21:59:26,025:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 21:59:26,083:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:00:28,876:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:00:28,894:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:00:32,225:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:00:32,230:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:00:32,237:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:00:32,327:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:00:32,340:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:00:34,535:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:00:34,545:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:01:05,372:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:01:05,383:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:01:06,755:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:01:06,767:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:01:06,854:INFO:Calculating mean and std
2024-09-13 22:01:07,016:INFO:Creating metrics dataframe
2024-09-13 22:01:07,154:INFO:Uploading results into container
2024-09-13 22:01:07,170:INFO:Uploading model into container now
2024-09-13 22:01:07,185:INFO:_master_model_container: 19
2024-09-13 22:01:07,185:INFO:_display_container: 3
2024-09-13 22:01:07,185:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=6887, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-13 22:01:07,185:INFO:create_model() successfully completed......................................
2024-09-13 22:01:09,133:INFO:SubProcess create_model() end ==================================
2024-09-13 22:01:09,133:INFO:Creating metrics dataframe
2024-09-13 22:01:09,166:INFO:Initializing Ridge Classifier
2024-09-13 22:01:09,166:INFO:Total runtime is 17.83918970823288 minutes
2024-09-13 22:01:09,183:INFO:SubProcess create_model() called ==================================
2024-09-13 22:01:09,183:INFO:Initializing create_model()
2024-09-13 22:01:09,183:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CE810570D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 22:01:09,183:INFO:Checking exceptions
2024-09-13 22:01:09,183:INFO:Importing libraries
2024-09-13 22:01:09,183:INFO:Copying training dataset
2024-09-13 22:01:10,599:INFO:Defining folds
2024-09-13 22:01:10,599:INFO:Declaring metric variables
2024-09-13 22:01:10,610:INFO:Importing untrained model
2024-09-13 22:01:10,620:INFO:Ridge Classifier Imported successfully
2024-09-13 22:01:10,636:INFO:Starting cross validation
2024-09-13 22:01:10,659:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 22:01:10,666:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 22:01:54,130:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:01:54,146:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:01:55,513:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:01:55,573:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:01:55,581:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:01:55,930:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:01:55,936:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:02:38,312:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:02:38,330:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:02:38,851:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:02:38,862:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:02:40,467:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:02:40,747:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:02:57,793:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:02:58,210:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:02:58,308:INFO:Calculating mean and std
2024-09-13 22:02:58,443:INFO:Creating metrics dataframe
2024-09-13 22:02:58,576:INFO:Uploading results into container
2024-09-13 22:02:58,587:INFO:Uploading model into container now
2024-09-13 22:02:58,595:INFO:_master_model_container: 20
2024-09-13 22:02:58,595:INFO:_display_container: 3
2024-09-13 22:02:58,609:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=6887, solver='auto',
                tol=0.0001)
2024-09-13 22:02:58,609:INFO:create_model() successfully completed......................................
2024-09-13 22:03:00,508:INFO:SubProcess create_model() end ==================================
2024-09-13 22:03:00,508:INFO:Creating metrics dataframe
2024-09-13 22:03:00,558:INFO:Initializing Random Forest Classifier
2024-09-13 22:03:00,558:INFO:Total runtime is 19.695719035466514 minutes
2024-09-13 22:03:00,558:INFO:SubProcess create_model() called ==================================
2024-09-13 22:03:00,558:INFO:Initializing create_model()
2024-09-13 22:03:00,558:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CE810570D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 22:03:00,558:INFO:Checking exceptions
2024-09-13 22:03:00,558:INFO:Importing libraries
2024-09-13 22:03:00,558:INFO:Copying training dataset
2024-09-13 22:03:02,091:INFO:Defining folds
2024-09-13 22:03:02,091:INFO:Declaring metric variables
2024-09-13 22:03:02,103:INFO:Importing untrained model
2024-09-13 22:03:02,109:INFO:Random Forest Classifier Imported successfully
2024-09-13 22:03:02,144:INFO:Starting cross validation
2024-09-13 22:03:02,191:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 22:03:02,209:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 22:03:44,593:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:03:44,606:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:03:45,326:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:03:45,692:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:03:45,920:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:04:18,510:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:04:20,023:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:04:22,143:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:04:22,285:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:04:39,318:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:04:39,323:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:04:40,262:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:04:40,334:INFO:Calculating mean and std
2024-09-13 22:04:40,417:INFO:Creating metrics dataframe
2024-09-13 22:04:40,515:INFO:Uploading results into container
2024-09-13 22:04:40,522:INFO:Uploading model into container now
2024-09-13 22:04:40,534:INFO:_master_model_container: 21
2024-09-13 22:04:40,534:INFO:_display_container: 3
2024-09-13 22:04:40,534:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6887, verbose=0,
                       warm_start=False)
2024-09-13 22:04:40,534:INFO:create_model() successfully completed......................................
2024-09-13 22:04:42,216:INFO:SubProcess create_model() end ==================================
2024-09-13 22:04:42,216:INFO:Creating metrics dataframe
2024-09-13 22:04:42,251:INFO:Initializing Quadratic Discriminant Analysis
2024-09-13 22:04:42,251:INFO:Total runtime is 21.390595638751986 minutes
2024-09-13 22:04:42,251:INFO:SubProcess create_model() called ==================================
2024-09-13 22:04:42,251:INFO:Initializing create_model()
2024-09-13 22:04:42,251:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CE810570D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 22:04:42,251:INFO:Checking exceptions
2024-09-13 22:04:42,251:INFO:Importing libraries
2024-09-13 22:04:42,266:INFO:Copying training dataset
2024-09-13 22:04:44,050:INFO:Defining folds
2024-09-13 22:04:44,050:INFO:Declaring metric variables
2024-09-13 22:04:44,060:INFO:Importing untrained model
2024-09-13 22:04:44,071:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-13 22:04:44,088:INFO:Starting cross validation
2024-09-13 22:04:44,108:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 22:04:44,117:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 22:05:02,719:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 22:05:03,560:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 22:05:04,701:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 22:05:05,315:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 22:05:36,450:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:05:38,752:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:05:38,763:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:05:38,986:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:05:39,008:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:05:39,372:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:05:39,382:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:05:39,382:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:05:49,815:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 22:05:52,203:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 22:05:53,031:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 22:05:53,650:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 22:06:21,036:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:06:21,046:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:06:25,070:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:06:25,084:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:06:25,648:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:06:26,198:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:06:31,380:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 22:06:34,793:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-13 22:06:49,158:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:06:51,854:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:06:51,890:INFO:Calculating mean and std
2024-09-13 22:06:51,908:INFO:Creating metrics dataframe
2024-09-13 22:06:51,944:INFO:Uploading results into container
2024-09-13 22:06:51,944:INFO:Uploading model into container now
2024-09-13 22:06:51,944:INFO:_master_model_container: 22
2024-09-13 22:06:51,944:INFO:_display_container: 3
2024-09-13 22:06:51,944:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-13 22:06:51,944:INFO:create_model() successfully completed......................................
2024-09-13 22:06:52,474:INFO:SubProcess create_model() end ==================================
2024-09-13 22:06:52,474:INFO:Creating metrics dataframe
2024-09-13 22:06:52,491:INFO:Initializing Ada Boost Classifier
2024-09-13 22:06:52,491:INFO:Total runtime is 23.56126877864202 minutes
2024-09-13 22:06:52,491:INFO:SubProcess create_model() called ==================================
2024-09-13 22:06:52,491:INFO:Initializing create_model()
2024-09-13 22:06:52,491:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CE810570D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 22:06:52,491:INFO:Checking exceptions
2024-09-13 22:06:52,491:INFO:Importing libraries
2024-09-13 22:06:52,491:INFO:Copying training dataset
2024-09-13 22:06:53,727:INFO:Defining folds
2024-09-13 22:06:53,727:INFO:Declaring metric variables
2024-09-13 22:06:53,732:INFO:Importing untrained model
2024-09-13 22:06:53,746:INFO:Ada Boost Classifier Imported successfully
2024-09-13 22:06:53,760:INFO:Starting cross validation
2024-09-13 22:06:53,787:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 22:06:53,791:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 22:07:08,355:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 22:07:09,090:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 22:07:09,378:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 22:07:09,866:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 22:07:34,480:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:07:34,490:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:07:35,887:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:07:35,908:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:07:35,908:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:07:35,918:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:07:36,797:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:07:36,807:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:07:48,267:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 22:07:49,470:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 22:07:49,987:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 22:07:50,671:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 22:08:11,020:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:08:11,027:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:08:12,432:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:08:12,442:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:08:13,174:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:08:13,186:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:08:13,565:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:08:13,576:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:08:20,129:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 22:08:20,945:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-13 22:08:35,826:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:08:35,832:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:08:36,626:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:08:36,632:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:08:36,663:INFO:Calculating mean and std
2024-09-13 22:08:36,666:INFO:Creating metrics dataframe
2024-09-13 22:08:36,674:INFO:Uploading results into container
2024-09-13 22:08:36,676:INFO:Uploading model into container now
2024-09-13 22:08:36,678:INFO:_master_model_container: 23
2024-09-13 22:08:36,678:INFO:_display_container: 3
2024-09-13 22:08:36,679:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=6887)
2024-09-13 22:08:36,680:INFO:create_model() successfully completed......................................
2024-09-13 22:08:36,935:INFO:SubProcess create_model() end ==================================
2024-09-13 22:08:36,935:INFO:Creating metrics dataframe
2024-09-13 22:08:36,955:INFO:Initializing Gradient Boosting Classifier
2024-09-13 22:08:36,955:INFO:Total runtime is 25.30232749382655 minutes
2024-09-13 22:08:36,963:INFO:SubProcess create_model() called ==================================
2024-09-13 22:08:36,964:INFO:Initializing create_model()
2024-09-13 22:08:36,964:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CE810570D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 22:08:36,965:INFO:Checking exceptions
2024-09-13 22:08:36,966:INFO:Importing libraries
2024-09-13 22:08:36,967:INFO:Copying training dataset
2024-09-13 22:08:38,201:INFO:Defining folds
2024-09-13 22:08:38,201:INFO:Declaring metric variables
2024-09-13 22:08:38,209:INFO:Importing untrained model
2024-09-13 22:08:38,221:INFO:Gradient Boosting Classifier Imported successfully
2024-09-13 22:08:38,233:INFO:Starting cross validation
2024-09-13 22:08:38,255:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 22:08:38,259:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 22:10:57,847:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:10:59,158:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:10:59,539:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:11:00,281:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:11:00,290:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:13:38,134:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:13:38,143:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:13:39,206:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:13:39,229:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:13:40,587:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:13:41,027:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:15:28,323:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:15:29,339:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:15:29,443:INFO:Calculating mean and std
2024-09-13 22:15:29,570:INFO:Creating metrics dataframe
2024-09-13 22:15:29,693:INFO:Uploading results into container
2024-09-13 22:15:29,703:INFO:Uploading model into container now
2024-09-13 22:15:29,719:INFO:_master_model_container: 24
2024-09-13 22:15:29,719:INFO:_display_container: 3
2024-09-13 22:15:29,730:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6887, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-13 22:15:29,732:INFO:create_model() successfully completed......................................
2024-09-13 22:15:31,668:INFO:SubProcess create_model() end ==================================
2024-09-13 22:15:31,668:INFO:Creating metrics dataframe
2024-09-13 22:15:31,705:INFO:Initializing Linear Discriminant Analysis
2024-09-13 22:15:31,705:INFO:Total runtime is 32.2148343205452 minutes
2024-09-13 22:15:31,711:INFO:SubProcess create_model() called ==================================
2024-09-13 22:15:31,712:INFO:Initializing create_model()
2024-09-13 22:15:31,713:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CE810570D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 22:15:31,713:INFO:Checking exceptions
2024-09-13 22:15:31,713:INFO:Importing libraries
2024-09-13 22:15:31,715:INFO:Copying training dataset
2024-09-13 22:15:33,251:INFO:Defining folds
2024-09-13 22:15:33,252:INFO:Declaring metric variables
2024-09-13 22:15:33,268:INFO:Importing untrained model
2024-09-13 22:15:33,282:INFO:Linear Discriminant Analysis Imported successfully
2024-09-13 22:15:33,307:INFO:Starting cross validation
2024-09-13 22:15:33,363:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 22:15:33,379:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 22:17:49,573:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:17:49,585:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:17:51,815:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:17:51,821:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:17:53,372:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:17:53,377:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:17:57,846:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:19:56,482:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:20:04,485:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:20:10,030:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:20:10,040:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:20:10,924:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:20:10,934:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:21:21,728:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:21:21,734:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:21:25,060:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:21:25,066:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:21:25,193:INFO:Calculating mean and std
2024-09-13 22:21:25,381:INFO:Creating metrics dataframe
2024-09-13 22:21:25,568:INFO:Uploading results into container
2024-09-13 22:21:25,583:INFO:Uploading model into container now
2024-09-13 22:21:25,622:INFO:_master_model_container: 25
2024-09-13 22:21:25,623:INFO:_display_container: 3
2024-09-13 22:21:25,633:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-13 22:21:25,634:INFO:create_model() successfully completed......................................
2024-09-13 22:21:27,981:INFO:SubProcess create_model() end ==================================
2024-09-13 22:21:27,981:INFO:Creating metrics dataframe
2024-09-13 22:21:28,025:INFO:Initializing Extra Trees Classifier
2024-09-13 22:21:28,027:INFO:Total runtime is 38.15353769858679 minutes
2024-09-13 22:21:28,037:INFO:SubProcess create_model() called ==================================
2024-09-13 22:21:28,037:INFO:Initializing create_model()
2024-09-13 22:21:28,038:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CE810570D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 22:21:28,038:INFO:Checking exceptions
2024-09-13 22:21:28,038:INFO:Importing libraries
2024-09-13 22:21:28,041:INFO:Copying training dataset
2024-09-13 22:21:30,049:INFO:Defining folds
2024-09-13 22:21:30,049:INFO:Declaring metric variables
2024-09-13 22:21:30,060:INFO:Importing untrained model
2024-09-13 22:21:30,067:INFO:Extra Trees Classifier Imported successfully
2024-09-13 22:21:30,081:INFO:Starting cross validation
2024-09-13 22:21:30,101:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 22:21:30,111:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 22:22:16,229:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:22:16,690:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:22:16,701:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:22:16,845:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:22:17,432:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:22:55,072:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:22:56,200:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:22:57,122:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:22:57,664:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:23:16,812:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:23:17,371:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:23:17,422:INFO:Calculating mean and std
2024-09-13 22:23:17,497:INFO:Creating metrics dataframe
2024-09-13 22:23:17,564:INFO:Uploading results into container
2024-09-13 22:23:17,573:INFO:Uploading model into container now
2024-09-13 22:23:17,582:INFO:_master_model_container: 26
2024-09-13 22:23:17,582:INFO:_display_container: 3
2024-09-13 22:23:17,585:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=6887, verbose=0,
                     warm_start=False)
2024-09-13 22:23:17,585:INFO:create_model() successfully completed......................................
2024-09-13 22:23:19,033:INFO:SubProcess create_model() end ==================================
2024-09-13 22:23:19,033:INFO:Creating metrics dataframe
2024-09-13 22:23:19,065:INFO:Initializing Light Gradient Boosting Machine
2024-09-13 22:23:19,065:INFO:Total runtime is 40.00417160987855 minutes
2024-09-13 22:23:19,073:INFO:SubProcess create_model() called ==================================
2024-09-13 22:23:19,073:INFO:Initializing create_model()
2024-09-13 22:23:19,073:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CE810570D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 22:23:19,074:INFO:Checking exceptions
2024-09-13 22:23:19,074:INFO:Importing libraries
2024-09-13 22:23:19,076:INFO:Copying training dataset
2024-09-13 22:23:20,538:INFO:Defining folds
2024-09-13 22:23:20,539:INFO:Declaring metric variables
2024-09-13 22:23:20,549:INFO:Importing untrained model
2024-09-13 22:23:20,559:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-13 22:23:20,581:INFO:Starting cross validation
2024-09-13 22:23:20,615:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 22:23:20,627:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 22:24:01,927:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:24:02,044:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:24:02,420:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:24:02,456:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:24:32,557:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:24:33,752:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:24:35,531:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:24:36,077:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:24:51,021:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:24:51,049:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:24:51,115:INFO:Calculating mean and std
2024-09-13 22:24:51,201:INFO:Creating metrics dataframe
2024-09-13 22:24:51,274:INFO:Uploading results into container
2024-09-13 22:24:51,280:INFO:Uploading model into container now
2024-09-13 22:24:51,290:INFO:_master_model_container: 27
2024-09-13 22:24:51,290:INFO:_display_container: 3
2024-09-13 22:24:51,294:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6887, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-13 22:24:51,295:INFO:create_model() successfully completed......................................
2024-09-13 22:24:53,218:INFO:SubProcess create_model() end ==================================
2024-09-13 22:24:53,219:INFO:Creating metrics dataframe
2024-09-13 22:24:53,272:INFO:Initializing Dummy Classifier
2024-09-13 22:24:53,272:INFO:Total runtime is 41.57428134282431 minutes
2024-09-13 22:24:53,283:INFO:SubProcess create_model() called ==================================
2024-09-13 22:24:53,284:INFO:Initializing create_model()
2024-09-13 22:24:53,284:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CE810570D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 22:24:53,284:INFO:Checking exceptions
2024-09-13 22:24:53,284:INFO:Importing libraries
2024-09-13 22:24:53,291:INFO:Copying training dataset
2024-09-13 22:24:54,837:INFO:Defining folds
2024-09-13 22:24:54,837:INFO:Declaring metric variables
2024-09-13 22:24:54,849:INFO:Importing untrained model
2024-09-13 22:24:54,855:INFO:Dummy Classifier Imported successfully
2024-09-13 22:24:54,869:INFO:Starting cross validation
2024-09-13 22:24:54,897:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 22:24:54,909:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 22:25:19,681:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:25:19,690:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:25:20,809:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:25:20,820:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:25:21,989:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:25:22,160:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:25:22,179:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:25:45,880:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:25:47,082:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:25:48,016:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:25:48,025:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:25:48,896:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:25:48,906:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:26:00,464:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:26:00,469:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:26:01,055:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-13 22:26:01,060:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:26:01,085:INFO:Calculating mean and std
2024-09-13 22:26:01,088:INFO:Creating metrics dataframe
2024-09-13 22:26:01,095:INFO:Uploading results into container
2024-09-13 22:26:01,096:INFO:Uploading model into container now
2024-09-13 22:26:01,096:INFO:_master_model_container: 28
2024-09-13 22:26:01,097:INFO:_display_container: 3
2024-09-13 22:26:01,097:INFO:DummyClassifier(constant=None, random_state=6887, strategy='prior')
2024-09-13 22:26:01,097:INFO:create_model() successfully completed......................................
2024-09-13 22:26:01,339:INFO:SubProcess create_model() end ==================================
2024-09-13 22:26:01,339:INFO:Creating metrics dataframe
2024-09-13 22:26:01,380:INFO:Initializing create_model()
2024-09-13 22:26:01,381:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6887, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 22:26:01,381:INFO:Checking exceptions
2024-09-13 22:26:01,389:INFO:Importing libraries
2024-09-13 22:26:01,389:INFO:Copying training dataset
2024-09-13 22:26:02,205:INFO:Defining folds
2024-09-13 22:26:02,205:INFO:Declaring metric variables
2024-09-13 22:26:02,206:INFO:Importing untrained model
2024-09-13 22:26:02,206:INFO:Declaring custom model
2024-09-13 22:26:02,207:INFO:Random Forest Classifier Imported successfully
2024-09-13 22:26:02,223:INFO:Cross validation set to False
2024-09-13 22:26:02,223:INFO:Fitting Model
2024-09-13 22:26:10,131:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6887, verbose=0,
                       warm_start=False)
2024-09-13 22:26:10,131:INFO:create_model() successfully completed......................................
2024-09-13 22:26:10,426:INFO:_master_model_container: 28
2024-09-13 22:26:10,426:INFO:_display_container: 3
2024-09-13 22:26:10,427:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6887, verbose=0,
                       warm_start=False)
2024-09-13 22:26:10,427:INFO:compare_models() successfully completed......................................
2024-09-13 22:35:18,962:INFO:Initializing compare_models()
2024-09-13 22:35:18,962:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-13 22:35:18,963:INFO:Checking exceptions
2024-09-13 22:35:19,894:INFO:Preparing display monitor
2024-09-13 22:35:19,977:INFO:Initializing Logistic Regression
2024-09-13 22:35:19,977:INFO:Total runtime is 0.0 minutes
2024-09-13 22:35:19,996:INFO:SubProcess create_model() called ==================================
2024-09-13 22:35:19,996:INFO:Initializing create_model()
2024-09-13 22:35:19,997:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEF6EB79D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-13 22:35:19,997:INFO:Checking exceptions
2024-09-13 22:35:19,997:INFO:Importing libraries
2024-09-13 22:35:19,997:INFO:Copying training dataset
2024-09-13 22:35:21,911:INFO:Defining folds
2024-09-13 22:35:21,911:INFO:Declaring metric variables
2024-09-13 22:35:21,930:INFO:Importing untrained model
2024-09-13 22:35:21,950:INFO:Logistic Regression Imported successfully
2024-09-13 22:35:21,975:INFO:Starting cross validation
2024-09-13 22:35:22,012:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-13 22:35:22,033:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-13 22:38:43,478:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 22:38:50,412:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 22:38:52,995:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 22:38:55,189:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-13 22:39:01,802:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:39:01,844:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-13 22:39:08,891:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:39:12,096:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:39:14,850:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-13 22:39:14,951:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:42:06,158:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-14 09:42:21,029:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-14 09:42:27,718:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-14 09:42:27,784:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-14 09:42:37,569:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:42:37,618:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:42:47,827:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:42:47,861:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:42:49,861:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:42:49,884:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:42:50,596:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:42:50,613:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:44:41,810:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-14 09:44:44,811:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-14 09:44:53,523:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:44:53,553:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:44:55,389:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:44:55,589:INFO:Calculating mean and std
2024-09-14 09:44:55,752:INFO:Creating metrics dataframe
2024-09-14 09:44:55,921:INFO:Uploading results into container
2024-09-14 09:44:55,937:INFO:Uploading model into container now
2024-09-14 09:44:55,952:INFO:_master_model_container: 29
2024-09-14 09:44:55,952:INFO:_display_container: 4
2024-09-14 09:44:55,968:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=6887, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-14 09:44:55,968:INFO:create_model() successfully completed......................................
2024-09-14 09:44:59,138:INFO:SubProcess create_model() end ==================================
2024-09-14 09:44:59,138:INFO:Creating metrics dataframe
2024-09-14 09:44:59,160:INFO:Initializing K Neighbors Classifier
2024-09-14 09:44:59,160:INFO:Total runtime is 669.6530424555143 minutes
2024-09-14 09:44:59,175:INFO:SubProcess create_model() called ==================================
2024-09-14 09:44:59,175:INFO:Initializing create_model()
2024-09-14 09:44:59,175:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEF6EB79D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-14 09:44:59,175:INFO:Checking exceptions
2024-09-14 09:44:59,175:INFO:Importing libraries
2024-09-14 09:44:59,175:INFO:Copying training dataset
2024-09-14 09:45:01,744:INFO:Defining folds
2024-09-14 09:45:01,744:INFO:Declaring metric variables
2024-09-14 09:45:01,759:INFO:Importing untrained model
2024-09-14 09:45:01,775:INFO:K Neighbors Classifier Imported successfully
2024-09-14 09:45:01,775:INFO:Starting cross validation
2024-09-14 09:45:01,813:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-14 09:45:01,828:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-14 09:45:49,884:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:45:51,050:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:45:51,794:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:45:52,793:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:46:26,746:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:46:28,174:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:46:28,404:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:46:30,298:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:46:50,253:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:46:51,549:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:46:51,609:INFO:Calculating mean and std
2024-09-14 09:46:51,650:INFO:Creating metrics dataframe
2024-09-14 09:46:51,670:INFO:Uploading results into container
2024-09-14 09:46:51,680:INFO:Uploading model into container now
2024-09-14 09:46:51,680:INFO:_master_model_container: 30
2024-09-14 09:46:51,680:INFO:_display_container: 4
2024-09-14 09:46:51,680:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-14 09:46:51,680:INFO:create_model() successfully completed......................................
2024-09-14 09:46:52,208:INFO:SubProcess create_model() end ==================================
2024-09-14 09:46:52,208:INFO:Creating metrics dataframe
2024-09-14 09:46:52,262:INFO:Initializing Naive Bayes
2024-09-14 09:46:52,262:INFO:Total runtime is 671.538081431389 minutes
2024-09-14 09:46:52,272:INFO:SubProcess create_model() called ==================================
2024-09-14 09:46:52,272:INFO:Initializing create_model()
2024-09-14 09:46:52,272:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEF6EB79D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-14 09:46:52,272:INFO:Checking exceptions
2024-09-14 09:46:52,272:INFO:Importing libraries
2024-09-14 09:46:52,280:INFO:Copying training dataset
2024-09-14 09:46:54,567:INFO:Defining folds
2024-09-14 09:46:54,567:INFO:Declaring metric variables
2024-09-14 09:46:54,577:INFO:Importing untrained model
2024-09-14 09:46:54,587:INFO:Naive Bayes Imported successfully
2024-09-14 09:46:54,607:INFO:Starting cross validation
2024-09-14 09:46:54,658:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-14 09:46:54,668:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-14 09:47:36,122:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:47:36,176:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:47:38,151:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:47:38,191:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:47:38,263:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:47:38,273:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:47:38,568:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:48:18,145:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:48:18,685:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:48:19,193:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:48:19,215:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:48:21,153:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:48:21,170:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:48:41,679:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:48:41,689:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:48:43,069:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:48:43,081:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:48:43,221:INFO:Calculating mean and std
2024-09-14 09:48:43,462:INFO:Creating metrics dataframe
2024-09-14 09:48:43,636:INFO:Uploading results into container
2024-09-14 09:48:43,656:INFO:Uploading model into container now
2024-09-14 09:48:43,687:INFO:_master_model_container: 31
2024-09-14 09:48:43,687:INFO:_display_container: 4
2024-09-14 09:48:43,697:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-14 09:48:43,697:INFO:create_model() successfully completed......................................
2024-09-14 09:48:47,282:INFO:SubProcess create_model() end ==================================
2024-09-14 09:48:47,290:INFO:Creating metrics dataframe
2024-09-14 09:48:47,363:INFO:Initializing Decision Tree Classifier
2024-09-14 09:48:47,363:INFO:Total runtime is 673.4564325014751 minutes
2024-09-14 09:48:47,373:INFO:SubProcess create_model() called ==================================
2024-09-14 09:48:47,373:INFO:Initializing create_model()
2024-09-14 09:48:47,373:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEF6EB79D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-14 09:48:47,373:INFO:Checking exceptions
2024-09-14 09:48:47,373:INFO:Importing libraries
2024-09-14 09:48:47,373:INFO:Copying training dataset
2024-09-14 09:48:51,593:INFO:Defining folds
2024-09-14 09:48:51,593:INFO:Declaring metric variables
2024-09-14 09:48:51,634:INFO:Importing untrained model
2024-09-14 09:48:51,652:INFO:Decision Tree Classifier Imported successfully
2024-09-14 09:48:51,674:INFO:Starting cross validation
2024-09-14 09:48:51,854:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-14 09:48:51,894:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-14 09:49:32,478:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:49:35,121:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:49:36,143:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:50:16,431:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:50:21,532:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:50:25,305:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:50:25,365:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:50:43,342:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:50:46,517:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:50:46,573:INFO:Calculating mean and std
2024-09-14 09:50:46,642:INFO:Creating metrics dataframe
2024-09-14 09:50:46,658:INFO:Uploading results into container
2024-09-14 09:50:46,658:INFO:Uploading model into container now
2024-09-14 09:50:46,658:INFO:_master_model_container: 32
2024-09-14 09:50:46,658:INFO:_display_container: 4
2024-09-14 09:50:46,674:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=6887, splitter='best')
2024-09-14 09:50:46,674:INFO:create_model() successfully completed......................................
2024-09-14 09:50:47,322:INFO:SubProcess create_model() end ==================================
2024-09-14 09:50:47,322:INFO:Creating metrics dataframe
2024-09-14 09:50:47,375:INFO:Initializing SVM - Linear Kernel
2024-09-14 09:50:47,375:INFO:Total runtime is 675.4566352844239 minutes
2024-09-14 09:50:47,375:INFO:SubProcess create_model() called ==================================
2024-09-14 09:50:47,375:INFO:Initializing create_model()
2024-09-14 09:50:47,375:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEF6EB79D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-14 09:50:47,375:INFO:Checking exceptions
2024-09-14 09:50:47,375:INFO:Importing libraries
2024-09-14 09:50:47,391:INFO:Copying training dataset
2024-09-14 09:50:49,571:INFO:Defining folds
2024-09-14 09:50:49,571:INFO:Declaring metric variables
2024-09-14 09:50:49,581:INFO:Importing untrained model
2024-09-14 09:50:49,592:INFO:SVM - Linear Kernel Imported successfully
2024-09-14 09:50:49,612:INFO:Starting cross validation
2024-09-14 09:50:49,661:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-14 09:50:49,673:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-14 09:51:47,809:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:51:47,817:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:51:48,856:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:51:48,877:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:51:50,399:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:51:50,409:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:51:50,502:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:51:50,533:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:52:40,802:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:52:40,811:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:52:41,641:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:52:41,652:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:52:44,603:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:52:44,607:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:52:44,611:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:52:45,296:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:52:45,347:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:53:15,137:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:53:15,147:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:53:17,453:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:53:17,463:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:53:17,527:INFO:Calculating mean and std
2024-09-14 09:53:17,581:INFO:Creating metrics dataframe
2024-09-14 09:53:17,627:INFO:Uploading results into container
2024-09-14 09:53:17,627:INFO:Uploading model into container now
2024-09-14 09:53:17,627:INFO:_master_model_container: 33
2024-09-14 09:53:17,627:INFO:_display_container: 4
2024-09-14 09:53:17,627:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=6887, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-14 09:53:17,643:INFO:create_model() successfully completed......................................
2024-09-14 09:53:18,055:INFO:SubProcess create_model() end ==================================
2024-09-14 09:53:18,055:INFO:Creating metrics dataframe
2024-09-14 09:53:18,129:INFO:Initializing Ridge Classifier
2024-09-14 09:53:18,129:INFO:Total runtime is 677.9691889921825 minutes
2024-09-14 09:53:18,129:INFO:SubProcess create_model() called ==================================
2024-09-14 09:53:18,129:INFO:Initializing create_model()
2024-09-14 09:53:18,129:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEF6EB79D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-14 09:53:18,129:INFO:Checking exceptions
2024-09-14 09:53:18,129:INFO:Importing libraries
2024-09-14 09:53:18,144:INFO:Copying training dataset
2024-09-14 09:53:22,564:INFO:Defining folds
2024-09-14 09:53:22,572:INFO:Declaring metric variables
2024-09-14 09:53:22,574:INFO:Importing untrained model
2024-09-14 09:53:22,594:INFO:Ridge Classifier Imported successfully
2024-09-14 09:53:22,613:INFO:Starting cross validation
2024-09-14 09:53:22,665:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-14 09:53:22,685:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-14 09:54:15,286:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:54:15,312:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:54:16,006:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:54:16,016:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:54:18,119:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:54:18,131:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:54:18,162:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:54:54,576:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:54:54,587:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:54:56,206:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:54:56,575:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:54:56,585:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:54:59,345:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:55:29,620:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:55:30,318:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:55:30,461:INFO:Calculating mean and std
2024-09-14 09:55:30,621:INFO:Creating metrics dataframe
2024-09-14 09:55:30,816:INFO:Uploading results into container
2024-09-14 09:55:30,831:INFO:Uploading model into container now
2024-09-14 09:55:30,847:INFO:_master_model_container: 34
2024-09-14 09:55:30,847:INFO:_display_container: 4
2024-09-14 09:55:30,863:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=6887, solver='auto',
                tol=0.0001)
2024-09-14 09:55:30,863:INFO:create_model() successfully completed......................................
2024-09-14 09:55:34,412:INFO:SubProcess create_model() end ==================================
2024-09-14 09:55:34,412:INFO:Creating metrics dataframe
2024-09-14 09:55:34,483:INFO:Initializing Random Forest Classifier
2024-09-14 09:55:34,483:INFO:Total runtime is 680.2417655189832 minutes
2024-09-14 09:55:34,493:INFO:SubProcess create_model() called ==================================
2024-09-14 09:55:34,493:INFO:Initializing create_model()
2024-09-14 09:55:34,493:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEF6EB79D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-14 09:55:34,493:INFO:Checking exceptions
2024-09-14 09:55:34,493:INFO:Importing libraries
2024-09-14 09:55:34,503:INFO:Copying training dataset
2024-09-14 09:55:38,006:INFO:Defining folds
2024-09-14 09:55:38,006:INFO:Declaring metric variables
2024-09-14 09:55:38,016:INFO:Importing untrained model
2024-09-14 09:55:38,026:INFO:Random Forest Classifier Imported successfully
2024-09-14 09:55:38,044:INFO:Starting cross validation
2024-09-14 09:55:38,097:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-14 09:55:38,127:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-14 09:56:17,615:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:56:17,628:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:56:24,504:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:56:26,076:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:56:26,559:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:56:59,218:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:57:05,791:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:57:07,045:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:57:08,662:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:57:32,500:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:57:32,510:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:57:37,002:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 09:57:37,114:INFO:Calculating mean and std
2024-09-14 09:57:37,262:INFO:Creating metrics dataframe
2024-09-14 09:57:37,400:INFO:Uploading results into container
2024-09-14 09:57:37,400:INFO:Uploading model into container now
2024-09-14 09:57:37,415:INFO:_master_model_container: 35
2024-09-14 09:57:37,415:INFO:_display_container: 4
2024-09-14 09:57:37,431:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6887, verbose=0,
                       warm_start=False)
2024-09-14 09:57:37,431:INFO:create_model() successfully completed......................................
2024-09-14 09:57:39,991:INFO:SubProcess create_model() end ==================================
2024-09-14 09:57:39,991:INFO:Creating metrics dataframe
2024-09-14 09:57:40,023:INFO:Initializing Quadratic Discriminant Analysis
2024-09-14 09:57:40,023:INFO:Total runtime is 682.3340903480848 minutes
2024-09-14 09:57:40,038:INFO:SubProcess create_model() called ==================================
2024-09-14 09:57:40,038:INFO:Initializing create_model()
2024-09-14 09:57:40,038:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEF6EB79D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-14 09:57:40,038:INFO:Checking exceptions
2024-09-14 09:57:40,038:INFO:Importing libraries
2024-09-14 09:57:40,038:INFO:Copying training dataset
2024-09-14 09:57:42,048:INFO:Defining folds
2024-09-14 09:57:42,048:INFO:Declaring metric variables
2024-09-14 09:57:42,064:INFO:Importing untrained model
2024-09-14 09:57:42,064:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-14 09:57:42,079:INFO:Starting cross validation
2024-09-14 09:57:42,128:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-14 09:57:42,128:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-14 09:58:02,942:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-14 09:58:03,787:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-14 09:58:04,125:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-14 09:58:05,211:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-14 09:58:38,111:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:58:42,602:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:58:42,617:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:58:42,779:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:58:42,790:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:58:42,795:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:58:43,294:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:58:43,304:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:58:56,591:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-14 09:58:59,713:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-14 09:59:02,346:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-14 09:59:02,955:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-14 09:59:31,429:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:59:31,531:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:59:37,929:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:59:37,947:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 09:59:43,975:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:59:45,253:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 09:59:51,041:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-14 09:59:54,082:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-14 10:00:18,050:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:00:20,901:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:00:21,025:INFO:Calculating mean and std
2024-09-14 10:00:21,178:INFO:Creating metrics dataframe
2024-09-14 10:00:21,325:INFO:Uploading results into container
2024-09-14 10:00:21,341:INFO:Uploading model into container now
2024-09-14 10:00:21,357:INFO:_master_model_container: 36
2024-09-14 10:00:21,357:INFO:_display_container: 4
2024-09-14 10:00:21,372:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-14 10:00:21,372:INFO:create_model() successfully completed......................................
2024-09-14 10:00:23,901:INFO:SubProcess create_model() end ==================================
2024-09-14 10:00:23,901:INFO:Creating metrics dataframe
2024-09-14 10:00:23,932:INFO:Initializing Ada Boost Classifier
2024-09-14 10:00:23,932:INFO:Total runtime is 685.0659147262573 minutes
2024-09-14 10:00:23,948:INFO:SubProcess create_model() called ==================================
2024-09-14 10:00:23,948:INFO:Initializing create_model()
2024-09-14 10:00:23,948:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEF6EB79D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-14 10:00:23,948:INFO:Checking exceptions
2024-09-14 10:00:23,948:INFO:Importing libraries
2024-09-14 10:00:23,948:INFO:Copying training dataset
2024-09-14 10:00:26,157:INFO:Defining folds
2024-09-14 10:00:26,157:INFO:Declaring metric variables
2024-09-14 10:00:26,165:INFO:Importing untrained model
2024-09-14 10:00:26,175:INFO:Ada Boost Classifier Imported successfully
2024-09-14 10:00:26,190:INFO:Starting cross validation
2024-09-14 10:00:26,232:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-14 10:00:26,232:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-14 10:00:43,418:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-14 10:00:43,988:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-14 10:00:44,401:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-14 10:00:45,408:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-14 10:01:09,313:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:01:09,324:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:01:10,401:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:01:10,412:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:01:10,617:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:01:10,628:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:01:12,676:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:01:12,705:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:01:26,284:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-14 10:01:26,784:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-14 10:01:26,895:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-14 10:01:28,956:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-14 10:01:51,550:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:01:51,552:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:01:51,723:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:01:51,745:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:01:52,162:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:01:52,174:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:01:53,629:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:01:53,640:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:02:03,865:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-14 10:02:04,455:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-14 10:02:24,018:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:02:24,028:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:02:24,829:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:02:24,839:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:02:24,912:INFO:Calculating mean and std
2024-09-14 10:02:25,003:INFO:Creating metrics dataframe
2024-09-14 10:02:25,047:INFO:Uploading results into container
2024-09-14 10:02:25,062:INFO:Uploading model into container now
2024-09-14 10:02:25,062:INFO:_master_model_container: 37
2024-09-14 10:02:25,062:INFO:_display_container: 4
2024-09-14 10:02:25,062:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=6887)
2024-09-14 10:02:25,062:INFO:create_model() successfully completed......................................
2024-09-14 10:02:26,598:INFO:SubProcess create_model() end ==================================
2024-09-14 10:02:26,598:INFO:Creating metrics dataframe
2024-09-14 10:02:26,640:INFO:Initializing Gradient Boosting Classifier
2024-09-14 10:02:26,640:INFO:Total runtime is 687.1110501050949 minutes
2024-09-14 10:02:26,640:INFO:SubProcess create_model() called ==================================
2024-09-14 10:02:26,640:INFO:Initializing create_model()
2024-09-14 10:02:26,640:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEF6EB79D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-14 10:02:26,640:INFO:Checking exceptions
2024-09-14 10:02:26,640:INFO:Importing libraries
2024-09-14 10:02:26,640:INFO:Copying training dataset
2024-09-14 10:02:27,976:INFO:Defining folds
2024-09-14 10:02:27,976:INFO:Declaring metric variables
2024-09-14 10:02:27,984:INFO:Importing untrained model
2024-09-14 10:02:27,994:INFO:Gradient Boosting Classifier Imported successfully
2024-09-14 10:02:28,007:INFO:Starting cross validation
2024-09-14 10:02:28,047:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-14 10:02:28,055:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-14 10:05:03,507:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:05:05,817:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:05:07,097:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:05:07,585:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:05:07,595:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:07:20,024:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:07:20,035:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:07:22,450:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:07:22,460:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:07:23,851:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:07:25,583:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:09:18,535:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:09:20,183:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:09:20,270:INFO:Calculating mean and std
2024-09-14 10:09:20,402:INFO:Creating metrics dataframe
2024-09-14 10:09:20,518:INFO:Uploading results into container
2024-09-14 10:09:20,524:INFO:Uploading model into container now
2024-09-14 10:09:20,540:INFO:_master_model_container: 38
2024-09-14 10:09:20,540:INFO:_display_container: 4
2024-09-14 10:09:20,540:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6887, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-14 10:09:20,540:INFO:create_model() successfully completed......................................
2024-09-14 10:09:22,724:INFO:SubProcess create_model() end ==================================
2024-09-14 10:09:22,724:INFO:Creating metrics dataframe
2024-09-14 10:09:22,761:INFO:Initializing Linear Discriminant Analysis
2024-09-14 10:09:22,761:INFO:Total runtime is 694.046400431792 minutes
2024-09-14 10:09:22,761:INFO:SubProcess create_model() called ==================================
2024-09-14 10:09:22,761:INFO:Initializing create_model()
2024-09-14 10:09:22,761:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEF6EB79D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-14 10:09:22,761:INFO:Checking exceptions
2024-09-14 10:09:22,761:INFO:Importing libraries
2024-09-14 10:09:22,777:INFO:Copying training dataset
2024-09-14 10:09:24,281:INFO:Defining folds
2024-09-14 10:09:24,297:INFO:Declaring metric variables
2024-09-14 10:09:24,297:INFO:Importing untrained model
2024-09-14 10:09:24,297:INFO:Linear Discriminant Analysis Imported successfully
2024-09-14 10:09:24,312:INFO:Starting cross validation
2024-09-14 10:09:24,350:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-14 10:09:24,350:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-14 10:11:04,134:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:11:06,708:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:11:06,716:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:11:06,869:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:11:06,871:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:11:07,016:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:11:07,026:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:12:45,061:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:12:51,557:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:12:53,815:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:12:53,817:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:12:53,950:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:12:53,960:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:13:53,425:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:13:53,427:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:13:58,541:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-14 10:13:58,551:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:13:58,728:INFO:Calculating mean and std
2024-09-14 10:13:58,902:INFO:Creating metrics dataframe
2024-09-14 10:13:59,075:INFO:Uploading results into container
2024-09-14 10:13:59,095:INFO:Uploading model into container now
2024-09-14 10:13:59,119:INFO:_master_model_container: 39
2024-09-14 10:13:59,119:INFO:_display_container: 4
2024-09-14 10:13:59,134:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-14 10:13:59,134:INFO:create_model() successfully completed......................................
2024-09-14 10:14:01,647:INFO:SubProcess create_model() end ==================================
2024-09-14 10:14:01,647:INFO:Creating metrics dataframe
2024-09-14 10:14:01,690:INFO:Initializing Extra Trees Classifier
2024-09-14 10:14:01,690:INFO:Total runtime is 698.6952044844628 minutes
2024-09-14 10:14:01,690:INFO:SubProcess create_model() called ==================================
2024-09-14 10:14:01,690:INFO:Initializing create_model()
2024-09-14 10:14:01,690:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEF6EB79D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-14 10:14:01,690:INFO:Checking exceptions
2024-09-14 10:14:01,690:INFO:Importing libraries
2024-09-14 10:14:01,705:INFO:Copying training dataset
2024-09-14 10:14:03,407:INFO:Defining folds
2024-09-14 10:14:03,407:INFO:Declaring metric variables
2024-09-14 10:14:03,407:INFO:Importing untrained model
2024-09-14 10:14:03,419:INFO:Extra Trees Classifier Imported successfully
2024-09-14 10:14:03,439:INFO:Starting cross validation
2024-09-14 10:14:03,490:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-14 10:14:03,500:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-14 10:14:46,740:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:14:48,273:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:14:48,281:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:14:48,284:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:14:48,917:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:15:23,981:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:15:26,796:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:15:27,959:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:15:28,111:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:15:47,496:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:15:50,631:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:15:50,699:INFO:Calculating mean and std
2024-09-14 10:15:50,800:INFO:Creating metrics dataframe
2024-09-14 10:15:50,870:INFO:Uploading results into container
2024-09-14 10:15:50,870:INFO:Uploading model into container now
2024-09-14 10:15:50,885:INFO:_master_model_container: 40
2024-09-14 10:15:50,885:INFO:_display_container: 4
2024-09-14 10:15:50,885:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=6887, verbose=0,
                     warm_start=False)
2024-09-14 10:15:50,885:INFO:create_model() successfully completed......................................
2024-09-14 10:15:52,538:INFO:SubProcess create_model() end ==================================
2024-09-14 10:15:52,538:INFO:Creating metrics dataframe
2024-09-14 10:15:52,585:INFO:Initializing Light Gradient Boosting Machine
2024-09-14 10:15:52,585:INFO:Total runtime is 700.5434573531152 minutes
2024-09-14 10:15:52,585:INFO:SubProcess create_model() called ==================================
2024-09-14 10:15:52,600:INFO:Initializing create_model()
2024-09-14 10:15:52,600:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEF6EB79D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-14 10:15:52,600:INFO:Checking exceptions
2024-09-14 10:15:52,600:INFO:Importing libraries
2024-09-14 10:15:52,600:INFO:Copying training dataset
2024-09-14 10:15:53,788:INFO:Defining folds
2024-09-14 10:15:53,788:INFO:Declaring metric variables
2024-09-14 10:15:53,804:INFO:Importing untrained model
2024-09-14 10:15:53,819:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-14 10:15:53,835:INFO:Starting cross validation
2024-09-14 10:15:53,874:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-14 10:15:53,884:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-14 10:16:24,238:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:16:25,168:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:16:25,988:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:16:28,591:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:16:50,226:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:16:51,758:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:16:52,349:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:16:54,426:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:17:12,084:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:17:13,376:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:17:13,418:INFO:Calculating mean and std
2024-09-14 10:17:13,426:INFO:Creating metrics dataframe
2024-09-14 10:17:13,436:INFO:Uploading results into container
2024-09-14 10:17:13,438:INFO:Uploading model into container now
2024-09-14 10:17:13,438:INFO:_master_model_container: 41
2024-09-14 10:17:13,438:INFO:_display_container: 4
2024-09-14 10:17:13,438:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6887, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-14 10:17:13,438:INFO:create_model() successfully completed......................................
2024-09-14 10:17:13,921:INFO:SubProcess create_model() end ==================================
2024-09-14 10:17:13,921:INFO:Creating metrics dataframe
2024-09-14 10:17:13,952:INFO:Initializing Dummy Classifier
2024-09-14 10:17:13,952:INFO:Total runtime is 701.8995824615162 minutes
2024-09-14 10:17:13,952:INFO:SubProcess create_model() called ==================================
2024-09-14 10:17:13,952:INFO:Initializing create_model()
2024-09-14 10:17:13,952:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CEF6EB79D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-14 10:17:13,952:INFO:Checking exceptions
2024-09-14 10:17:13,952:INFO:Importing libraries
2024-09-14 10:17:13,952:INFO:Copying training dataset
2024-09-14 10:17:15,447:INFO:Defining folds
2024-09-14 10:17:15,447:INFO:Declaring metric variables
2024-09-14 10:17:15,455:INFO:Importing untrained model
2024-09-14 10:17:15,457:INFO:Dummy Classifier Imported successfully
2024-09-14 10:17:15,477:INFO:Starting cross validation
2024-09-14 10:17:15,528:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-14 10:17:15,528:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-14 10:17:39,448:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:17:39,459:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:17:40,051:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:17:40,061:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:17:40,885:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:17:40,896:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:17:41,508:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:18:01,739:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:18:02,847:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:18:03,600:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:18:03,611:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:18:04,183:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:18:04,193:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:18:22,992:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:18:23,001:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:18:24,048:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-14 10:18:24,048:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-14 10:18:24,079:INFO:Calculating mean and std
2024-09-14 10:18:24,079:INFO:Creating metrics dataframe
2024-09-14 10:18:24,089:INFO:Uploading results into container
2024-09-14 10:18:24,097:INFO:Uploading model into container now
2024-09-14 10:18:24,099:INFO:_master_model_container: 42
2024-09-14 10:18:24,099:INFO:_display_container: 4
2024-09-14 10:18:24,099:INFO:DummyClassifier(constant=None, random_state=6887, strategy='prior')
2024-09-14 10:18:24,099:INFO:create_model() successfully completed......................................
2024-09-14 10:18:24,574:INFO:SubProcess create_model() end ==================================
2024-09-14 10:18:24,574:INFO:Creating metrics dataframe
2024-09-14 10:18:24,633:INFO:Initializing create_model()
2024-09-14 10:18:24,633:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CEF4B75E10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6887, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-14 10:18:24,633:INFO:Checking exceptions
2024-09-14 10:18:24,643:INFO:Importing libraries
2024-09-14 10:18:24,643:INFO:Copying training dataset
2024-09-14 10:18:25,959:INFO:Defining folds
2024-09-14 10:18:25,959:INFO:Declaring metric variables
2024-09-14 10:18:25,959:INFO:Importing untrained model
2024-09-14 10:18:25,959:INFO:Declaring custom model
2024-09-14 10:18:25,959:INFO:Random Forest Classifier Imported successfully
2024-09-14 10:18:26,007:INFO:Cross validation set to False
2024-09-14 10:18:26,007:INFO:Fitting Model
2024-09-14 10:18:39,519:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6887, verbose=0,
                       warm_start=False)
2024-09-14 10:18:39,519:INFO:create_model() successfully completed......................................
2024-09-14 10:18:39,904:INFO:_master_model_container: 42
2024-09-14 10:18:39,904:INFO:_display_container: 4
2024-09-14 10:18:39,912:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6887, verbose=0,
                       warm_start=False)
2024-09-14 10:18:39,912:INFO:compare_models() successfully completed......................................
2024-09-15 18:16:23,320:INFO:gpu_param set to False
2024-09-15 18:16:24,218:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:16:24,264:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:16:25,651:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:16:25,658:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:17:14,745:INFO:PyCaret ClassificationExperiment
2024-09-15 18:17:14,745:INFO:Logging name: clf-default-name
2024-09-15 18:17:14,745:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-15 18:17:14,745:INFO:version 3.3.2
2024-09-15 18:17:14,745:INFO:Initializing setup()
2024-09-15 18:17:14,745:INFO:self.USI: 6e52
2024-09-15 18:17:14,745:INFO:self._variable_keys: {'X_test', 'memory', 'fold_groups_param', 'fix_imbalance', 'exp_id', 'X', 'gpu_n_jobs_param', 'log_plots_param', 'target_param', 'y', 'X_train', 'fold_generator', 'is_multiclass', 'y_train', '_ml_usecase', 'logging_param', 'html_param', 'y_test', 'USI', 'seed', 'fold_shuffle_param', 'gpu_param', 'idx', 'n_jobs_param', 'exp_name_log', 'pipeline', 'data', '_available_plots'}
2024-09-15 18:17:14,745:INFO:Checking environment
2024-09-15 18:17:14,745:INFO:python_version: 3.11.9
2024-09-15 18:17:14,745:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-09-15 18:17:14,745:INFO:machine: AMD64
2024-09-15 18:17:14,745:INFO:platform: Windows-10-10.0.19045-SP0
2024-09-15 18:17:14,775:INFO:Memory: svmem(total=6302064640, available=873525248, percent=86.1, used=5428539392, free=873525248)
2024-09-15 18:17:14,775:INFO:Physical Core: 2
2024-09-15 18:17:14,775:INFO:Logical Core: 4
2024-09-15 18:17:14,775:INFO:Checking libraries
2024-09-15 18:17:14,775:INFO:System:
2024-09-15 18:17:14,775:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-09-15 18:17:14,775:INFO:executable: c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Scripts\python.exe
2024-09-15 18:17:14,775:INFO:   machine: Windows-10-10.0.19045-SP0
2024-09-15 18:17:14,775:INFO:PyCaret required dependencies:
2024-09-15 18:17:14,775:INFO:                 pip: 24.2
2024-09-15 18:17:14,775:INFO:          setuptools: 74.1.2
2024-09-15 18:17:14,775:INFO:             pycaret: 3.3.2
2024-09-15 18:17:14,775:INFO:             IPython: 8.27.0
2024-09-15 18:17:14,775:INFO:          ipywidgets: 8.1.5
2024-09-15 18:17:14,775:INFO:                tqdm: 4.66.5
2024-09-15 18:17:14,775:INFO:               numpy: 1.26.4
2024-09-15 18:17:14,775:INFO:              pandas: 2.1.4
2024-09-15 18:17:14,775:INFO:              jinja2: 3.1.4
2024-09-15 18:17:14,775:INFO:               scipy: 1.11.4
2024-09-15 18:17:14,775:INFO:              joblib: 1.3.2
2024-09-15 18:17:14,775:INFO:             sklearn: 1.4.2
2024-09-15 18:17:14,775:INFO:                pyod: 2.0.2
2024-09-15 18:17:14,775:INFO:            imblearn: 0.12.3
2024-09-15 18:17:14,775:INFO:   category_encoders: 2.6.3
2024-09-15 18:17:14,775:INFO:            lightgbm: 4.5.0
2024-09-15 18:17:14,775:INFO:               numba: 0.60.0
2024-09-15 18:17:14,775:INFO:            requests: 2.32.3
2024-09-15 18:17:14,775:INFO:          matplotlib: 3.7.5
2024-09-15 18:17:14,775:INFO:          scikitplot: 0.3.7
2024-09-15 18:17:14,775:INFO:         yellowbrick: 1.5
2024-09-15 18:17:14,775:INFO:              plotly: 5.24.1
2024-09-15 18:17:14,775:INFO:    plotly-resampler: Not installed
2024-09-15 18:17:14,775:INFO:             kaleido: 0.2.1
2024-09-15 18:17:14,775:INFO:           schemdraw: 0.15
2024-09-15 18:17:14,775:INFO:         statsmodels: 0.14.2
2024-09-15 18:17:14,775:INFO:              sktime: 0.26.0
2024-09-15 18:17:14,775:INFO:               tbats: 1.1.3
2024-09-15 18:17:14,790:INFO:            pmdarima: 2.0.4
2024-09-15 18:17:14,790:INFO:              psutil: 6.0.0
2024-09-15 18:17:14,790:INFO:          markupsafe: 2.1.5
2024-09-15 18:17:14,791:INFO:             pickle5: Not installed
2024-09-15 18:17:14,791:INFO:         cloudpickle: 3.0.0
2024-09-15 18:17:14,791:INFO:         deprecation: 2.1.0
2024-09-15 18:17:14,791:INFO:              xxhash: 3.5.0
2024-09-15 18:17:14,791:INFO:           wurlitzer: Not installed
2024-09-15 18:17:14,791:INFO:PyCaret optional dependencies:
2024-09-15 18:17:14,791:INFO:                shap: Not installed
2024-09-15 18:17:14,791:INFO:           interpret: Not installed
2024-09-15 18:17:14,791:INFO:                umap: Not installed
2024-09-15 18:17:14,791:INFO:     ydata_profiling: Not installed
2024-09-15 18:17:14,791:INFO:  explainerdashboard: Not installed
2024-09-15 18:17:14,791:INFO:             autoviz: Not installed
2024-09-15 18:17:14,791:INFO:           fairlearn: Not installed
2024-09-15 18:17:14,791:INFO:          deepchecks: Not installed
2024-09-15 18:17:14,791:INFO:             xgboost: Not installed
2024-09-15 18:17:14,791:INFO:            catboost: Not installed
2024-09-15 18:17:14,791:INFO:              kmodes: Not installed
2024-09-15 18:17:14,791:INFO:             mlxtend: Not installed
2024-09-15 18:17:14,791:INFO:       statsforecast: Not installed
2024-09-15 18:17:14,791:INFO:        tune_sklearn: Not installed
2024-09-15 18:17:14,791:INFO:                 ray: Not installed
2024-09-15 18:17:14,791:INFO:            hyperopt: Not installed
2024-09-15 18:17:14,791:INFO:              optuna: Not installed
2024-09-15 18:17:14,791:INFO:               skopt: Not installed
2024-09-15 18:17:14,791:INFO:              mlflow: Not installed
2024-09-15 18:17:14,791:INFO:              gradio: Not installed
2024-09-15 18:17:14,791:INFO:             fastapi: Not installed
2024-09-15 18:17:14,791:INFO:             uvicorn: Not installed
2024-09-15 18:17:14,791:INFO:              m2cgen: Not installed
2024-09-15 18:17:14,791:INFO:           evidently: Not installed
2024-09-15 18:17:14,791:INFO:               fugue: Not installed
2024-09-15 18:17:14,791:INFO:           streamlit: Not installed
2024-09-15 18:17:14,791:INFO:             prophet: Not installed
2024-09-15 18:17:14,791:INFO:None
2024-09-15 18:17:14,791:INFO:Set up data.
2024-09-15 18:17:18,491:INFO:Set up folding strategy.
2024-09-15 18:17:18,494:INFO:Set up train/test split.
2024-09-15 18:17:19,643:INFO:Set up index.
2024-09-15 18:17:19,710:INFO:Assigning column types.
2024-09-15 18:17:23,665:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-15 18:17:23,773:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-15 18:17:23,775:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-15 18:17:23,824:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:17:23,825:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:17:23,940:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-15 18:17:23,940:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-15 18:17:24,000:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:17:24,000:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:17:24,000:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-15 18:17:24,079:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-15 18:17:24,122:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:17:24,130:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:17:24,193:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-15 18:17:24,241:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:17:24,241:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:17:24,241:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-15 18:17:24,357:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:17:24,357:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:17:24,457:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:17:24,457:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:17:24,473:INFO:Preparing preprocessing pipeline...
2024-09-15 18:17:24,557:INFO:Set up simple imputation.
2024-09-15 18:17:24,676:INFO:Set up column name cleaning.
2024-09-15 18:17:29,042:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:249: UserWarning: Persisting input arguments took 1.85s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  fitted_transformer = self._memory_fit(

2024-09-15 18:17:40,375:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:256: UserWarning: Persisting input arguments took 2.03s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_transform(

2024-09-15 18:17:41,176:INFO:Finished creating preprocessing pipeline.
2024-09-15 18:17:41,224:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\jonat\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'severity', 'version',
                                             'assigned_to_detail.id',
                                             'Psummary_intermittent',
                                             'Psummary_bug',
                                             'Psummary_tracking',
                                             'Psummary_single', 'Psummary_sync',
                                             'Psummary_pr', 'Psummary_wptsync',
                                             'Psummary_test', 'Psummary_w...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-15 18:17:41,224:INFO:Creating final display dataframe.
2024-09-15 18:17:49,250:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:289: UserWarning: Persisting input arguments took 3.08s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-09-15 18:18:02,401:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:111: UserWarning: Persisting input arguments took 1.37s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = pipeline._memory_transform(transformer, X, y)

2024-09-15 18:18:05,407:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:289: UserWarning: Persisting input arguments took 2.22s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-09-15 18:18:09,813:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3           Original data shape   
4        Transformed data shape   
5   Transformed train set shape   
6    Transformed test set shape   
7              Numeric features   
8                    Preprocess   
9               Imputation type   
10           Numeric imputation   
11       Categorical imputation   
12               Fold Generator   
13                  Fold Number   
14                     CPU Jobs   
15                      Use GPU   
16               Log Experiment   
17              Experiment Name   
18                          USI   

                                                Value  
0                                                5943  
1   assigned_to_detail.email_zhaojiazhong-hf@loong...  
2                                              Binary  
3                                        (4109, 4184)  
4                                        (4109, 4184)  
5                                        (2876, 4184)  
6                                        (1233, 4184)  
7                                                3133  
8                                                True  
9                                              simple  
10                                               mean  
11                                               mode  
12                                    StratifiedKFold  
13                                                 10  
14                                                 -1  
15                                              False  
16                                              False  
17                                   clf-default-name  
18                                               6e52  
2024-09-15 18:18:10,227:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:18:10,227:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:18:10,360:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:18:10,360:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:18:10,373:INFO:setup() successfully completed in 55.74s...............
2024-09-15 18:18:10,373:INFO:gpu_param set to False
2024-09-15 18:18:10,522:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:18:10,523:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:18:10,667:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:18:10,668:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:18:59,231:INFO:PyCaret ClassificationExperiment
2024-09-15 18:18:59,233:INFO:Logging name: clf-default-name
2024-09-15 18:18:59,233:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-15 18:18:59,233:INFO:version 3.3.2
2024-09-15 18:18:59,233:INFO:Initializing setup()
2024-09-15 18:18:59,233:INFO:self.USI: 0080
2024-09-15 18:18:59,234:INFO:self._variable_keys: {'X_test', 'memory', 'fold_groups_param', 'fix_imbalance', 'exp_id', 'X', 'gpu_n_jobs_param', 'log_plots_param', 'target_param', 'y', 'X_train', 'fold_generator', 'is_multiclass', 'y_train', '_ml_usecase', 'logging_param', 'html_param', 'y_test', 'USI', 'seed', 'fold_shuffle_param', 'gpu_param', 'idx', 'n_jobs_param', 'exp_name_log', 'pipeline', 'data', '_available_plots'}
2024-09-15 18:18:59,235:INFO:Checking environment
2024-09-15 18:18:59,235:INFO:python_version: 3.11.9
2024-09-15 18:18:59,235:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-09-15 18:18:59,235:INFO:machine: AMD64
2024-09-15 18:18:59,235:INFO:platform: Windows-10-10.0.19045-SP0
2024-09-15 18:18:59,240:INFO:Memory: svmem(total=6302064640, available=646045696, percent=89.7, used=5656018944, free=646045696)
2024-09-15 18:18:59,241:INFO:Physical Core: 2
2024-09-15 18:18:59,241:INFO:Logical Core: 4
2024-09-15 18:18:59,241:INFO:Checking libraries
2024-09-15 18:18:59,241:INFO:System:
2024-09-15 18:18:59,242:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-09-15 18:18:59,242:INFO:executable: c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Scripts\python.exe
2024-09-15 18:18:59,242:INFO:   machine: Windows-10-10.0.19045-SP0
2024-09-15 18:18:59,242:INFO:PyCaret required dependencies:
2024-09-15 18:18:59,243:INFO:                 pip: 24.2
2024-09-15 18:18:59,243:INFO:          setuptools: 74.1.2
2024-09-15 18:18:59,243:INFO:             pycaret: 3.3.2
2024-09-15 18:18:59,243:INFO:             IPython: 8.27.0
2024-09-15 18:18:59,243:INFO:          ipywidgets: 8.1.5
2024-09-15 18:18:59,243:INFO:                tqdm: 4.66.5
2024-09-15 18:18:59,243:INFO:               numpy: 1.26.4
2024-09-15 18:18:59,244:INFO:              pandas: 2.1.4
2024-09-15 18:18:59,244:INFO:              jinja2: 3.1.4
2024-09-15 18:18:59,244:INFO:               scipy: 1.11.4
2024-09-15 18:18:59,244:INFO:              joblib: 1.3.2
2024-09-15 18:18:59,244:INFO:             sklearn: 1.4.2
2024-09-15 18:18:59,244:INFO:                pyod: 2.0.2
2024-09-15 18:18:59,245:INFO:            imblearn: 0.12.3
2024-09-15 18:18:59,245:INFO:   category_encoders: 2.6.3
2024-09-15 18:18:59,245:INFO:            lightgbm: 4.5.0
2024-09-15 18:18:59,245:INFO:               numba: 0.60.0
2024-09-15 18:18:59,245:INFO:            requests: 2.32.3
2024-09-15 18:18:59,245:INFO:          matplotlib: 3.7.5
2024-09-15 18:18:59,259:INFO:          scikitplot: 0.3.7
2024-09-15 18:18:59,259:INFO:         yellowbrick: 1.5
2024-09-15 18:18:59,259:INFO:              plotly: 5.24.1
2024-09-15 18:18:59,259:INFO:    plotly-resampler: Not installed
2024-09-15 18:18:59,260:INFO:             kaleido: 0.2.1
2024-09-15 18:18:59,268:INFO:           schemdraw: 0.15
2024-09-15 18:18:59,268:INFO:         statsmodels: 0.14.2
2024-09-15 18:18:59,268:INFO:              sktime: 0.26.0
2024-09-15 18:18:59,268:INFO:               tbats: 1.1.3
2024-09-15 18:18:59,268:INFO:            pmdarima: 2.0.4
2024-09-15 18:18:59,268:INFO:              psutil: 6.0.0
2024-09-15 18:18:59,268:INFO:          markupsafe: 2.1.5
2024-09-15 18:18:59,268:INFO:             pickle5: Not installed
2024-09-15 18:18:59,268:INFO:         cloudpickle: 3.0.0
2024-09-15 18:18:59,268:INFO:         deprecation: 2.1.0
2024-09-15 18:18:59,268:INFO:              xxhash: 3.5.0
2024-09-15 18:18:59,268:INFO:           wurlitzer: Not installed
2024-09-15 18:18:59,268:INFO:PyCaret optional dependencies:
2024-09-15 18:18:59,268:INFO:                shap: Not installed
2024-09-15 18:18:59,268:INFO:           interpret: Not installed
2024-09-15 18:18:59,268:INFO:                umap: Not installed
2024-09-15 18:18:59,268:INFO:     ydata_profiling: Not installed
2024-09-15 18:18:59,268:INFO:  explainerdashboard: Not installed
2024-09-15 18:18:59,268:INFO:             autoviz: Not installed
2024-09-15 18:18:59,268:INFO:           fairlearn: Not installed
2024-09-15 18:18:59,268:INFO:          deepchecks: Not installed
2024-09-15 18:18:59,268:INFO:             xgboost: Not installed
2024-09-15 18:18:59,268:INFO:            catboost: Not installed
2024-09-15 18:18:59,268:INFO:              kmodes: Not installed
2024-09-15 18:18:59,268:INFO:             mlxtend: Not installed
2024-09-15 18:18:59,268:INFO:       statsforecast: Not installed
2024-09-15 18:18:59,268:INFO:        tune_sklearn: Not installed
2024-09-15 18:18:59,268:INFO:                 ray: Not installed
2024-09-15 18:18:59,268:INFO:            hyperopt: Not installed
2024-09-15 18:18:59,268:INFO:              optuna: Not installed
2024-09-15 18:18:59,268:INFO:               skopt: Not installed
2024-09-15 18:18:59,268:INFO:              mlflow: Not installed
2024-09-15 18:18:59,268:INFO:              gradio: Not installed
2024-09-15 18:18:59,268:INFO:             fastapi: Not installed
2024-09-15 18:18:59,268:INFO:             uvicorn: Not installed
2024-09-15 18:18:59,284:INFO:              m2cgen: Not installed
2024-09-15 18:18:59,284:INFO:           evidently: Not installed
2024-09-15 18:18:59,284:INFO:               fugue: Not installed
2024-09-15 18:18:59,284:INFO:           streamlit: Not installed
2024-09-15 18:18:59,284:INFO:             prophet: Not installed
2024-09-15 18:18:59,284:INFO:None
2024-09-15 18:18:59,284:INFO:Set up data.
2024-09-15 18:19:01,706:INFO:Set up folding strategy.
2024-09-15 18:19:01,706:INFO:Set up train/test split.
2024-09-15 18:19:03,205:INFO:Set up index.
2024-09-15 18:19:03,221:INFO:Assigning column types.
2024-09-15 18:19:03,906:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-15 18:19:04,084:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-15 18:19:04,084:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-15 18:19:04,206:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:04,206:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:04,356:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-15 18:19:04,356:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-15 18:19:04,406:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:04,406:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:04,406:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-15 18:19:04,498:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-15 18:19:04,552:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:04,552:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:04,621:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-15 18:19:04,671:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:04,671:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:04,671:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-15 18:19:04,784:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:04,784:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:04,900:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:04,900:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:04,900:INFO:Preparing preprocessing pipeline...
2024-09-15 18:19:05,286:INFO:Set up simple imputation.
2024-09-15 18:19:05,338:INFO:Set up column name cleaning.
2024-09-15 18:19:09,531:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:249: UserWarning: Persisting input arguments took 1.71s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  fitted_transformer = self._memory_fit(

2024-09-15 18:19:19,432:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:256: UserWarning: Persisting input arguments took 1.47s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_transform(

2024-09-15 18:19:20,232:INFO:Finished creating preprocessing pipeline.
2024-09-15 18:19:20,266:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\jonat\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'version',
                                             'assigned_to_detail.id',
                                             'Psummary_intermittent',
                                             'Psummary_bug',
                                             'Psummary_tracking',
                                             'Psummary_single', 'Psummary_sync',
                                             'Psummary_pr', 'Psummary_wptsync',
                                             'Psummary_test', 'Psummary_wpt',
                                             'Psumma...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-15 18:19:20,266:INFO:Creating final display dataframe.
2024-09-15 18:19:25,949:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:289: UserWarning: Persisting input arguments took 1.63s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-09-15 18:19:37,666:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:111: UserWarning: Persisting input arguments took 1.71s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = pipeline._memory_transform(transformer, X, y)

2024-09-15 18:19:40,169:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:111: UserWarning: Persisting input arguments took 0.63s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = pipeline._memory_transform(transformer, X, y)

2024-09-15 18:19:42,603:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:289: UserWarning: Persisting input arguments took 2.20s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-09-15 18:19:48,966:INFO:Setup _display_container:                     Description             Value
0                    Session id              7657
1                        Target          severity
2                   Target type        Multiclass
3           Original data shape      (4109, 4184)
4        Transformed data shape      (4109, 4184)
5   Transformed train set shape      (2876, 4184)
6    Transformed test set shape      (1233, 4184)
7              Numeric features              3132
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              0080
2024-09-15 18:19:49,218:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:49,219:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:49,470:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:49,471:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 18:19:49,475:INFO:setup() successfully completed in 50.26s...............
2024-09-15 18:19:50,676:INFO:Initializing create_model()
2024-09-15 18:19:50,676:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CE846C60D0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-15 18:19:50,676:INFO:Checking exceptions
2024-09-15 20:04:14,748:INFO:gpu_param set to False
2024-09-15 20:04:15,851:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 20:04:15,869:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 20:04:16,303:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 20:04:16,306:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 21:04:39,251:INFO:gpu_param set to False
2024-09-15 21:04:39,840:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 21:04:39,870:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 21:04:40,102:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 21:04:40,102:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-15 21:23:15,829:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-15 21:28:13,533:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-18 16:47:49,329:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-18 16:47:49,330:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-18 16:47:49,330:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-18 16:47:49,330:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-18 16:47:52,780:INFO:PyCaret ClassificationExperiment
2024-09-18 16:47:52,780:INFO:Logging name: clf-default-name
2024-09-18 16:47:52,780:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-18 16:47:52,780:INFO:version 3.3.2
2024-09-18 16:47:52,780:INFO:Initializing setup()
2024-09-18 16:47:52,780:INFO:self.USI: 4233
2024-09-18 16:47:52,780:INFO:self._variable_keys: {'_available_plots', 'html_param', 'seed', 'data', 'idx', 'target_param', 'fold_shuffle_param', 'gpu_n_jobs_param', 'fix_imbalance', 'memory', 'y_train', 'X', '_ml_usecase', 'exp_name_log', 'pipeline', 'y', 'X_train', 'USI', 'gpu_param', 'y_test', 'X_test', 'logging_param', 'is_multiclass', 'log_plots_param', 'fold_groups_param', 'fold_generator', 'exp_id', 'n_jobs_param'}
2024-09-18 16:47:52,780:INFO:Checking environment
2024-09-18 16:47:52,780:INFO:python_version: 3.11.9
2024-09-18 16:47:52,781:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-09-18 16:47:52,781:INFO:machine: AMD64
2024-09-18 16:47:52,781:INFO:platform: Windows-10-10.0.19045-SP0
2024-09-18 16:47:52,785:INFO:Memory: svmem(total=6302064640, available=489508864, percent=92.2, used=5812555776, free=489508864)
2024-09-18 16:47:52,785:INFO:Physical Core: 2
2024-09-18 16:47:52,785:INFO:Logical Core: 4
2024-09-18 16:47:52,785:INFO:Checking libraries
2024-09-18 16:47:52,785:INFO:System:
2024-09-18 16:47:52,785:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-09-18 16:47:52,786:INFO:executable: c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Scripts\python.exe
2024-09-18 16:47:52,786:INFO:   machine: Windows-10-10.0.19045-SP0
2024-09-18 16:47:52,786:INFO:PyCaret required dependencies:
2024-09-18 16:47:53,839:INFO:                 pip: 24.2
2024-09-18 16:47:53,839:INFO:          setuptools: 74.1.2
2024-09-18 16:47:53,855:INFO:             pycaret: 3.3.2
2024-09-18 16:47:53,855:INFO:             IPython: 8.27.0
2024-09-18 16:47:53,855:INFO:          ipywidgets: 8.1.5
2024-09-18 16:47:53,855:INFO:                tqdm: 4.66.5
2024-09-18 16:47:53,855:INFO:               numpy: 1.26.4
2024-09-18 16:47:53,855:INFO:              pandas: 2.1.4
2024-09-18 16:47:53,855:INFO:              jinja2: 3.1.4
2024-09-18 16:47:53,855:INFO:               scipy: 1.11.4
2024-09-18 16:47:53,855:INFO:              joblib: 1.3.2
2024-09-18 16:47:53,855:INFO:             sklearn: 1.4.2
2024-09-18 16:47:53,855:INFO:                pyod: 2.0.2
2024-09-18 16:47:53,855:INFO:            imblearn: 0.12.3
2024-09-18 16:47:53,855:INFO:   category_encoders: 2.6.3
2024-09-18 16:47:53,855:INFO:            lightgbm: 4.5.0
2024-09-18 16:47:53,855:INFO:               numba: 0.60.0
2024-09-18 16:47:53,855:INFO:            requests: 2.32.3
2024-09-18 16:47:53,855:INFO:          matplotlib: 3.7.5
2024-09-18 16:47:53,855:INFO:          scikitplot: 0.3.7
2024-09-18 16:47:53,855:INFO:         yellowbrick: 1.5
2024-09-18 16:47:53,855:INFO:              plotly: 5.24.1
2024-09-18 16:47:53,855:INFO:    plotly-resampler: Not installed
2024-09-18 16:47:53,855:INFO:             kaleido: 0.2.1
2024-09-18 16:47:53,855:INFO:           schemdraw: 0.15
2024-09-18 16:47:53,855:INFO:         statsmodels: 0.14.2
2024-09-18 16:47:53,855:INFO:              sktime: 0.26.0
2024-09-18 16:47:53,855:INFO:               tbats: 1.1.3
2024-09-18 16:47:53,855:INFO:            pmdarima: 2.0.4
2024-09-18 16:47:53,855:INFO:              psutil: 6.0.0
2024-09-18 16:47:53,855:INFO:          markupsafe: 2.1.5
2024-09-18 16:47:53,855:INFO:             pickle5: Not installed
2024-09-18 16:47:53,855:INFO:         cloudpickle: 3.0.0
2024-09-18 16:47:53,855:INFO:         deprecation: 2.1.0
2024-09-18 16:47:53,855:INFO:              xxhash: 3.5.0
2024-09-18 16:47:53,855:INFO:           wurlitzer: Not installed
2024-09-18 16:47:53,855:INFO:PyCaret optional dependencies:
2024-09-18 16:48:05,507:INFO:                shap: 0.44.1
2024-09-18 16:48:05,507:INFO:           interpret: 0.6.3
2024-09-18 16:48:05,507:INFO:                umap: 0.5.6
2024-09-18 16:48:05,507:INFO:     ydata_profiling: 4.10.0
2024-09-18 16:48:05,507:INFO:  explainerdashboard: 0.4.7
2024-09-18 16:48:05,507:INFO:             autoviz: Not installed
2024-09-18 16:48:05,507:INFO:           fairlearn: 0.7.0
2024-09-18 16:48:05,507:INFO:          deepchecks: Not installed
2024-09-18 16:48:05,507:INFO:             xgboost: 2.1.1
2024-09-18 16:48:05,507:INFO:            catboost: 1.2.7
2024-09-18 16:48:05,507:INFO:              kmodes: 0.12.2
2024-09-18 16:48:05,507:INFO:             mlxtend: 0.23.1
2024-09-18 16:48:05,507:INFO:       statsforecast: 1.5.0
2024-09-18 16:48:05,507:INFO:        tune_sklearn: Not installed
2024-09-18 16:48:05,507:INFO:                 ray: Not installed
2024-09-18 16:48:05,507:INFO:            hyperopt: 0.2.7
2024-09-18 16:48:05,507:INFO:              optuna: 4.0.0
2024-09-18 16:48:05,507:INFO:               skopt: 0.10.2
2024-09-18 16:48:05,507:INFO:              mlflow: 2.16.1
2024-09-18 16:48:05,507:INFO:              gradio: 4.44.0
2024-09-18 16:48:05,507:INFO:             fastapi: 0.114.2
2024-09-18 16:48:05,507:INFO:             uvicorn: 0.30.6
2024-09-18 16:48:05,507:INFO:              m2cgen: 0.10.0
2024-09-18 16:48:05,507:INFO:           evidently: 0.4.37
2024-09-18 16:48:05,507:INFO:               fugue: 0.8.7
2024-09-18 16:48:05,507:INFO:           streamlit: Not installed
2024-09-18 16:48:05,507:INFO:             prophet: Not installed
2024-09-18 16:48:05,507:INFO:None
2024-09-18 16:48:05,507:INFO:Set up data.
2024-09-18 16:48:09,689:INFO:Set up folding strategy.
2024-09-18 16:48:09,704:INFO:Set up train/test split.
2024-09-18 16:49:37,961:INFO:PyCaret ClassificationExperiment
2024-09-18 16:49:37,962:INFO:Logging name: clf-default-name
2024-09-18 16:49:37,963:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-18 16:49:37,964:INFO:version 3.3.2
2024-09-18 16:49:37,964:INFO:Initializing setup()
2024-09-18 16:49:37,965:INFO:self.USI: 8203
2024-09-18 16:49:37,966:INFO:self._variable_keys: {'_available_plots', 'html_param', 'seed', 'data', 'idx', 'target_param', 'fold_shuffle_param', 'gpu_n_jobs_param', 'fix_imbalance', 'memory', 'y_train', 'X', '_ml_usecase', 'exp_name_log', 'pipeline', 'y', 'X_train', 'USI', 'gpu_param', 'y_test', 'X_test', 'logging_param', 'is_multiclass', 'log_plots_param', 'fold_groups_param', 'fold_generator', 'exp_id', 'n_jobs_param'}
2024-09-18 16:49:37,968:INFO:Checking environment
2024-09-18 16:49:37,969:INFO:python_version: 3.11.9
2024-09-18 16:49:37,969:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-09-18 16:49:37,970:INFO:machine: AMD64
2024-09-18 16:49:37,971:INFO:platform: Windows-10-10.0.19045-SP0
2024-09-18 16:49:37,977:INFO:Memory: svmem(total=6302064640, available=499822592, percent=92.1, used=5802242048, free=499822592)
2024-09-18 16:49:37,978:INFO:Physical Core: 2
2024-09-18 16:49:37,979:INFO:Logical Core: 4
2024-09-18 16:49:37,979:INFO:Checking libraries
2024-09-18 16:49:37,980:INFO:System:
2024-09-18 16:49:37,981:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-09-18 16:49:37,981:INFO:executable: c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Scripts\python.exe
2024-09-18 16:49:37,982:INFO:   machine: Windows-10-10.0.19045-SP0
2024-09-18 16:49:37,982:INFO:PyCaret required dependencies:
2024-09-18 16:49:37,983:INFO:                 pip: 24.2
2024-09-18 16:49:37,985:INFO:          setuptools: 74.1.2
2024-09-18 16:49:37,987:INFO:             pycaret: 3.3.2
2024-09-18 16:49:37,987:INFO:             IPython: 8.27.0
2024-09-18 16:49:37,989:INFO:          ipywidgets: 8.1.5
2024-09-18 16:49:37,990:INFO:                tqdm: 4.66.5
2024-09-18 16:49:37,991:INFO:               numpy: 1.26.4
2024-09-18 16:49:37,991:INFO:              pandas: 2.1.4
2024-09-18 16:49:37,993:INFO:              jinja2: 3.1.4
2024-09-18 16:49:37,994:INFO:               scipy: 1.11.4
2024-09-18 16:49:37,994:INFO:              joblib: 1.3.2
2024-09-18 16:49:37,995:INFO:             sklearn: 1.4.2
2024-09-18 16:49:37,995:INFO:                pyod: 2.0.2
2024-09-18 16:49:37,997:INFO:            imblearn: 0.12.3
2024-09-18 16:49:37,997:INFO:   category_encoders: 2.6.3
2024-09-18 16:49:37,998:INFO:            lightgbm: 4.5.0
2024-09-18 16:49:37,998:INFO:               numba: 0.60.0
2024-09-18 16:49:37,999:INFO:            requests: 2.32.3
2024-09-18 16:49:37,999:INFO:          matplotlib: 3.7.5
2024-09-18 16:49:37,999:INFO:          scikitplot: 0.3.7
2024-09-18 16:49:37,999:INFO:         yellowbrick: 1.5
2024-09-18 16:49:37,999:INFO:              plotly: 5.24.1
2024-09-18 16:49:38,001:INFO:    plotly-resampler: Not installed
2024-09-18 16:49:38,001:INFO:             kaleido: 0.2.1
2024-09-18 16:49:38,001:INFO:           schemdraw: 0.15
2024-09-18 16:49:38,002:INFO:         statsmodels: 0.14.2
2024-09-18 16:49:38,002:INFO:              sktime: 0.26.0
2024-09-18 16:49:38,002:INFO:               tbats: 1.1.3
2024-09-18 16:49:38,002:INFO:            pmdarima: 2.0.4
2024-09-18 16:49:38,003:INFO:              psutil: 6.0.0
2024-09-18 16:49:38,003:INFO:          markupsafe: 2.1.5
2024-09-18 16:49:38,004:INFO:             pickle5: Not installed
2024-09-18 16:49:38,004:INFO:         cloudpickle: 3.0.0
2024-09-18 16:49:38,004:INFO:         deprecation: 2.1.0
2024-09-18 16:49:38,004:INFO:              xxhash: 3.5.0
2024-09-18 16:49:38,005:INFO:           wurlitzer: Not installed
2024-09-18 16:49:38,006:INFO:PyCaret optional dependencies:
2024-09-18 16:49:38,006:INFO:                shap: 0.44.1
2024-09-18 16:49:38,006:INFO:           interpret: 0.6.3
2024-09-18 16:49:38,007:INFO:                umap: 0.5.6
2024-09-18 16:49:38,008:INFO:     ydata_profiling: 4.10.0
2024-09-18 16:49:38,008:INFO:  explainerdashboard: 0.4.7
2024-09-18 16:49:38,009:INFO:             autoviz: Not installed
2024-09-18 16:49:38,009:INFO:           fairlearn: 0.7.0
2024-09-18 16:49:38,009:INFO:          deepchecks: Not installed
2024-09-18 16:49:38,009:INFO:             xgboost: 2.1.1
2024-09-18 16:49:38,009:INFO:            catboost: 1.2.7
2024-09-18 16:49:38,010:INFO:              kmodes: 0.12.2
2024-09-18 16:49:38,010:INFO:             mlxtend: 0.23.1
2024-09-18 16:49:38,010:INFO:       statsforecast: 1.5.0
2024-09-18 16:49:38,011:INFO:        tune_sklearn: Not installed
2024-09-18 16:49:38,011:INFO:                 ray: Not installed
2024-09-18 16:49:38,011:INFO:            hyperopt: 0.2.7
2024-09-18 16:49:38,011:INFO:              optuna: 4.0.0
2024-09-18 16:49:38,012:INFO:               skopt: 0.10.2
2024-09-18 16:49:38,012:INFO:              mlflow: 2.16.1
2024-09-18 16:49:38,012:INFO:              gradio: 4.44.0
2024-09-18 16:49:38,013:INFO:             fastapi: 0.114.2
2024-09-18 16:49:38,014:INFO:             uvicorn: 0.30.6
2024-09-18 16:49:38,014:INFO:              m2cgen: 0.10.0
2024-09-18 16:49:38,014:INFO:           evidently: 0.4.37
2024-09-18 16:49:38,014:INFO:               fugue: 0.8.7
2024-09-18 16:49:38,014:INFO:           streamlit: Not installed
2024-09-18 16:49:38,017:INFO:             prophet: Not installed
2024-09-18 16:49:38,052:INFO:None
2024-09-18 16:49:38,052:INFO:Set up data.
2024-09-18 16:49:42,061:INFO:Set up folding strategy.
2024-09-18 16:49:42,061:INFO:Set up train/test split.
2024-09-18 16:50:25,492:WARNING:C:\Users\jonat\AppData\Local\Temp\ipykernel_19648\2966898723.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.
  mergeddata = pd.read_csv('dataset_20240907_plusP_Psummary_tfidf.csv')

2024-09-18 16:50:52,629:INFO:PyCaret ClassificationExperiment
2024-09-18 16:50:52,630:INFO:Logging name: clf-default-name
2024-09-18 16:50:52,632:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-18 16:50:52,633:INFO:version 3.3.2
2024-09-18 16:50:52,633:INFO:Initializing setup()
2024-09-18 16:50:52,634:INFO:self.USI: b2fb
2024-09-18 16:50:52,635:INFO:self._variable_keys: {'_available_plots', 'html_param', 'seed', 'data', 'idx', 'target_param', 'fold_shuffle_param', 'gpu_n_jobs_param', 'fix_imbalance', 'memory', 'y_train', 'X', '_ml_usecase', 'exp_name_log', 'pipeline', 'y', 'X_train', 'USI', 'gpu_param', 'y_test', 'X_test', 'logging_param', 'is_multiclass', 'log_plots_param', 'fold_groups_param', 'fold_generator', 'exp_id', 'n_jobs_param'}
2024-09-18 16:50:52,636:INFO:Checking environment
2024-09-18 16:50:52,636:INFO:python_version: 3.11.9
2024-09-18 16:50:52,636:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-09-18 16:50:52,636:INFO:machine: AMD64
2024-09-18 16:50:52,636:INFO:platform: Windows-10-10.0.19045-SP0
2024-09-18 16:50:52,636:INFO:Memory: svmem(total=6302064640, available=771346432, percent=87.8, used=5530718208, free=771346432)
2024-09-18 16:50:52,636:INFO:Physical Core: 2
2024-09-18 16:50:52,636:INFO:Logical Core: 4
2024-09-18 16:50:52,636:INFO:Checking libraries
2024-09-18 16:50:52,636:INFO:System:
2024-09-18 16:50:52,636:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-09-18 16:50:52,636:INFO:executable: c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Scripts\python.exe
2024-09-18 16:50:52,636:INFO:   machine: Windows-10-10.0.19045-SP0
2024-09-18 16:50:52,636:INFO:PyCaret required dependencies:
2024-09-18 16:50:52,644:INFO:                 pip: 24.2
2024-09-18 16:50:52,644:INFO:          setuptools: 74.1.2
2024-09-18 16:50:52,644:INFO:             pycaret: 3.3.2
2024-09-18 16:50:52,644:INFO:             IPython: 8.27.0
2024-09-18 16:50:52,646:INFO:          ipywidgets: 8.1.5
2024-09-18 16:50:52,646:INFO:                tqdm: 4.66.5
2024-09-18 16:50:52,646:INFO:               numpy: 1.26.4
2024-09-18 16:50:52,646:INFO:              pandas: 2.1.4
2024-09-18 16:50:52,646:INFO:              jinja2: 3.1.4
2024-09-18 16:50:52,647:INFO:               scipy: 1.11.4
2024-09-18 16:50:52,647:INFO:              joblib: 1.3.2
2024-09-18 16:50:52,648:INFO:             sklearn: 1.4.2
2024-09-18 16:50:52,648:INFO:                pyod: 2.0.2
2024-09-18 16:50:52,648:INFO:            imblearn: 0.12.3
2024-09-18 16:50:52,648:INFO:   category_encoders: 2.6.3
2024-09-18 16:50:52,648:INFO:            lightgbm: 4.5.0
2024-09-18 16:50:52,649:INFO:               numba: 0.60.0
2024-09-18 16:50:52,650:INFO:            requests: 2.32.3
2024-09-18 16:50:52,650:INFO:          matplotlib: 3.7.5
2024-09-18 16:50:52,651:INFO:          scikitplot: 0.3.7
2024-09-18 16:50:52,651:INFO:         yellowbrick: 1.5
2024-09-18 16:50:52,652:INFO:              plotly: 5.24.1
2024-09-18 16:50:52,652:INFO:    plotly-resampler: Not installed
2024-09-18 16:50:52,653:INFO:             kaleido: 0.2.1
2024-09-18 16:50:52,653:INFO:           schemdraw: 0.15
2024-09-18 16:50:52,653:INFO:         statsmodels: 0.14.2
2024-09-18 16:50:52,655:INFO:              sktime: 0.26.0
2024-09-18 16:50:52,655:INFO:               tbats: 1.1.3
2024-09-18 16:50:52,656:INFO:            pmdarima: 2.0.4
2024-09-18 16:50:52,662:INFO:              psutil: 6.0.0
2024-09-18 16:50:52,662:INFO:          markupsafe: 2.1.5
2024-09-18 16:50:52,662:INFO:             pickle5: Not installed
2024-09-18 16:50:52,662:INFO:         cloudpickle: 3.0.0
2024-09-18 16:50:52,662:INFO:         deprecation: 2.1.0
2024-09-18 16:50:52,662:INFO:              xxhash: 3.5.0
2024-09-18 16:50:52,662:INFO:           wurlitzer: Not installed
2024-09-18 16:50:52,662:INFO:PyCaret optional dependencies:
2024-09-18 16:50:52,662:INFO:                shap: 0.44.1
2024-09-18 16:50:52,662:INFO:           interpret: 0.6.3
2024-09-18 16:50:52,662:INFO:                umap: 0.5.6
2024-09-18 16:50:52,662:INFO:     ydata_profiling: 4.10.0
2024-09-18 16:50:52,662:INFO:  explainerdashboard: 0.4.7
2024-09-18 16:50:52,662:INFO:             autoviz: Not installed
2024-09-18 16:50:52,662:INFO:           fairlearn: 0.7.0
2024-09-18 16:50:52,662:INFO:          deepchecks: Not installed
2024-09-18 16:50:52,662:INFO:             xgboost: 2.1.1
2024-09-18 16:50:52,662:INFO:            catboost: 1.2.7
2024-09-18 16:50:52,662:INFO:              kmodes: 0.12.2
2024-09-18 16:50:52,662:INFO:             mlxtend: 0.23.1
2024-09-18 16:50:52,662:INFO:       statsforecast: 1.5.0
2024-09-18 16:50:52,662:INFO:        tune_sklearn: Not installed
2024-09-18 16:50:52,662:INFO:                 ray: Not installed
2024-09-18 16:50:52,677:INFO:            hyperopt: 0.2.7
2024-09-18 16:50:52,679:INFO:              optuna: 4.0.0
2024-09-18 16:50:52,679:INFO:               skopt: 0.10.2
2024-09-18 16:50:52,679:INFO:              mlflow: 2.16.1
2024-09-18 16:50:52,680:INFO:              gradio: 4.44.0
2024-09-18 16:50:52,680:INFO:             fastapi: 0.114.2
2024-09-18 16:50:52,680:INFO:             uvicorn: 0.30.6
2024-09-18 16:50:52,680:INFO:              m2cgen: 0.10.0
2024-09-18 16:50:52,680:INFO:           evidently: 0.4.37
2024-09-18 16:50:52,681:INFO:               fugue: 0.8.7
2024-09-18 16:50:52,681:INFO:           streamlit: Not installed
2024-09-18 16:50:52,681:INFO:             prophet: Not installed
2024-09-18 16:50:52,682:INFO:None
2024-09-18 16:50:52,684:INFO:Set up data.
2024-09-18 16:50:54,196:INFO:Set up folding strategy.
2024-09-18 16:50:54,196:INFO:Set up train/test split.
2024-09-18 16:50:56,260:INFO:Set up index.
2024-09-18 16:50:56,310:INFO:Assigning column types.
2024-09-18 16:50:57,658:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-18 16:50:57,881:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-18 16:50:57,900:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-18 16:50:58,019:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-18 16:50:58,019:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-18 16:50:59,467:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-18 16:50:59,467:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-18 16:50:59,522:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-18 16:50:59,529:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-18 16:50:59,531:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-18 16:50:59,606:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-18 16:50:59,667:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-18 16:50:59,683:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-18 16:50:59,768:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-18 16:50:59,807:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-18 16:50:59,807:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-18 16:50:59,807:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-18 16:50:59,951:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-18 16:50:59,953:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-18 16:51:00,139:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-18 16:51:00,139:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-18 16:51:00,154:INFO:Preparing preprocessing pipeline...
2024-09-18 16:51:00,239:INFO:Set up simple imputation.
2024-09-18 16:51:00,308:INFO:Set up column name cleaning.
2024-09-18 16:51:04,795:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:249: UserWarning: Persisting input arguments took 2.06s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  fitted_transformer = self._memory_fit(

2024-09-18 16:51:17,398:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:256: UserWarning: Persisting input arguments took 2.05s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_transform(

2024-09-18 16:51:18,415:INFO:Finished creating preprocessing pipeline.
2024-09-18 16:51:18,466:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\jonat\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'version',
                                             'assigned_to_detail.id',
                                             'Psummary_intermittent',
                                             'Psummary_bug',
                                             'Psummary_tracking',
                                             'Psummary_single', 'Psummary_sync',
                                             'Psummary_pr', 'Psummary_wptsync',
                                             'Psummary_test', 'Psummary_wpt',
                                             'Psumma...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-18 16:51:18,466:INFO:Creating final display dataframe.
2024-09-18 16:51:25,700:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:289: UserWarning: Persisting input arguments took 2.20s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-09-18 16:51:39,014:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:111: UserWarning: Persisting input arguments took 1.57s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = pipeline._memory_transform(transformer, X, y)

2024-09-18 16:51:41,982:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\pipeline.py:289: UserWarning: Persisting input arguments took 2.00s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-09-18 16:51:47,245:INFO:Setup _display_container:                     Description             Value
0                    Session id              5553
1                        Target          severity
2                   Target type        Multiclass
3           Original data shape      (4109, 4184)
4        Transformed data shape      (4109, 4184)
5   Transformed train set shape      (2876, 4184)
6    Transformed test set shape      (1233, 4184)
7              Numeric features              3132
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              b2fb
2024-09-18 16:51:47,483:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-18 16:51:47,483:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-18 16:51:47,628:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-18 16:51:47,644:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-18 16:51:47,644:INFO:setup() successfully completed in 55.04s...............
2024-09-18 16:51:47,644:INFO:Initializing compare_models()
2024-09-18 16:51:47,644:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9ECE07890>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001B9ECE07890>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-18 16:51:47,644:INFO:Checking exceptions
2024-09-18 16:51:48,215:INFO:Preparing display monitor
2024-09-18 16:51:48,500:INFO:Initializing Logistic Regression
2024-09-18 16:51:48,500:INFO:Total runtime is 0.0 minutes
2024-09-18 16:51:48,513:INFO:SubProcess create_model() called ==================================
2024-09-18 16:51:48,514:INFO:Initializing create_model()
2024-09-18 16:51:48,515:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9ECE07890>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FD7CD4D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-18 16:51:48,515:INFO:Checking exceptions
2024-09-18 16:51:48,515:INFO:Importing libraries
2024-09-18 16:51:48,515:INFO:Copying training dataset
2024-09-18 16:51:50,583:INFO:Defining folds
2024-09-18 16:51:50,583:INFO:Declaring metric variables
2024-09-18 16:51:50,595:INFO:Importing untrained model
2024-09-18 16:51:50,601:INFO:Logistic Regression Imported successfully
2024-09-18 16:51:50,649:INFO:Starting cross validation
2024-09-18 16:51:50,722:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-18 16:51:50,750:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.
  warnings.warn(

2024-09-18 16:55:01,988:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-18 16:55:06,818:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-18 16:55:07,180:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-18 16:55:09,333:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-18 16:55:14,777:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-18 16:55:19,611:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-18 16:55:19,673:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-18 16:55:19,696:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-18 16:55:19,712:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-18 16:55:22,193:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-18 16:55:22,211:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-18 16:58:13,440:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-18 16:58:17,679:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-18 16:58:19,567:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-18 16:58:19,984:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-18 16:58:26,657:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-18 16:58:26,679:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-18 16:58:33,003:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-18 16:58:33,017:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-18 16:58:34,369:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-18 16:58:34,382:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-18 16:58:35,602:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-18 16:58:35,616:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-18 16:59:46,772:WARNING:c:\Users\jonat\OneDrive - Whitecliffe College\IT9115 Project\Data\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

